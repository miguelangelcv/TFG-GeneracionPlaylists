{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/header.png\" alt=\"Logo UCLM-ESII\" align=\"right\">\n",
    "\n",
    "<br><br><br><br>\n",
    "<h2><font color=\"#92002A\" size=4>Trabajo Fin de Grado</font></h2>\n",
    "\n",
    "<h1><font color=\"#6B001F\" size=5>Generaci√≥n autom√°tica de playlist de canciones <br> mediante t√©cnicas de miner√≠a de datos</font></h1>\n",
    "<h2><font color=\"#92002A\" size=3>Parte 5 - Generaci√≥n del DataSet</font></h2>\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: right\">\n",
    "    <font color=\"#B20033\" size=3><strong>Autor</strong>: <em>Miguel √Ångel Cantero V√≠llora</em></font><br>\n",
    "    <br>\n",
    "    <font color=\"#B20033\" size=3><strong>Directores</strong>: <em>Jos√© Antonio G√°mez Mart√≠n</em></font><br>\n",
    "    <font color=\"#B20033\" size=3><em>Juan √Ångel Aledo S√°nchez</em></font><br>\n",
    "    <br>\n",
    "<font color=\"#B20033\" size=3>Grado en Ingenier√≠a Inform√°tica</font><br>\n",
    "<font color=\"#B20033\" size=2>Escuela Superior de Ingenier√≠a Inform√°tica | Universidad de Castilla-La Mancha</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#92002A\" size=5>√çndice</font></h2>\n",
    "\n",
    "<br>\n",
    "\n",
    "* [1. Introducci√≥n](#section1)\n",
    "* [2. Lectura de datos](#section2)\n",
    "* [3. Generaci√≥n del DataSet](#section3)\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permite establecer la anchura de la celda\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm as tqdm_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<a id=\"section1\"></a>\n",
    "## <font color=\"#92002A\">1 - Introducci√≥n</font>\n",
    "<br>\n",
    "\n",
    "Tras haber realizado una recopilaci√≥n de playlists en Spotify y haber estudiado el conjunto *‚ÄòMillion Playlist Dataset‚Äô*, donde hemos podido identificar y establecer diferentes criterios para filtrar los resultados de la b√∫squeda hasta quedarnos con con el n√∫mero de muestras establecido, ya podemos generar nuestro conjunto de datos.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> __Recordatorio__:\n",
    "Las playlists que tenemos desde la posici√≥n 1.000.000 en adelante, se emplear√°n para crear el conjunto de prueba.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "## <font color=\"#92002A\">2 - Lectura de datos</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "En este apartado, vamos a leer el DataSet que hemos generado con la informaci√≥n de listas de reproducci√≥n obtenidas. Recordemos que la lista de pistas perteneciente a cada playlist no est√° incluida.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales\n",
    "\n",
    "# Directorio donde almacenaremos el DataSet que vamos a generar\n",
    "MPD_PATH = 'MPD'\n",
    "\n",
    "# Directorio donde se encuentran almacenadas las playlists que \n",
    "# hemos descargado\n",
    "PLS_SETS_PATH = 'pls_sets'\n",
    "\n",
    "# Directorio donde almacenaremos las listas de pistas que hemos \n",
    "# obtenido del conjunto de playlists (PLS_SETS_PATH)\n",
    "TLS_SETS_PATH = 'track_lists_sets'\n",
    "\n",
    "# Directorio empleado para guardar/leer aquellos datos generados\n",
    "# que resultan de inter√©s para diferentes tareas\n",
    "DATA_PATH = 'data'\n",
    "\n",
    "# N√∫mero de listas que almacenaremos por fichero\n",
    "NUM_LISTS_FILE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playlists obtenidas: 1019338\n"
     ]
    }
   ],
   "source": [
    "# Ruta del fichero donde est√°n contenidas las playlists\n",
    "mpd_info_file = os.path.join(DATA_PATH, 'mpd_info_set.csv')\n",
    "\n",
    "if os.path.isfile(mpd_info_file):\n",
    "    df_mpd_info = pd.read_csv(mpd_info_file, sep=';', encoding='utf-8')\n",
    "    print(f\"Playlists obtenidas: {len(df_mpd_info)}\")\n",
    "else:\n",
    "    print(\"¬°ERROR!: Fichero 'mpd_info_set.csv' no encontrado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Mostramos las primeras filas del DataFrame para comprobar que los datos se han cargado correctamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>collaborative</th>\n",
       "      <th>name</th>\n",
       "      <th>num_tracks</th>\n",
       "      <th>num_followers</th>\n",
       "      <th>num_artists</th>\n",
       "      <th>num_albums</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>modified_at</th>\n",
       "      <th>num_edits</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7u1RhzyK1ykPQ1wfemcqoR</td>\n",
       "      <td>False</td>\n",
       "      <td>Low viscosity vibes</td>\n",
       "      <td>58</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>38</td>\n",
       "      <td>15191903</td>\n",
       "      <td>1559722465</td>\n",
       "      <td>14</td>\n",
       "      <td>pls-set_07672.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0ZcZqlDEw4eLILfHdni8vG</td>\n",
       "      <td>False</td>\n",
       "      <td>dalanda üêâ</td>\n",
       "      <td>104</td>\n",
       "      <td>4</td>\n",
       "      <td>92</td>\n",
       "      <td>100</td>\n",
       "      <td>22361466</td>\n",
       "      <td>1558851313</td>\n",
       "      <td>75</td>\n",
       "      <td>pls-set_05170.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0Z48DgMwRPZVVTF7Y8RLB6</td>\n",
       "      <td>False</td>\n",
       "      <td>freeze pops</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>10643802</td>\n",
       "      <td>1552263755</td>\n",
       "      <td>7</td>\n",
       "      <td>pls-set_06675.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1G8V8nWYp6uVacS8XAPSbo</td>\n",
       "      <td>False</td>\n",
       "      <td>Golden Oldies</td>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>90</td>\n",
       "      <td>25418594</td>\n",
       "      <td>1556948665</td>\n",
       "      <td>34</td>\n",
       "      <td>pls-set_01145.zip</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id  collaborative                 name  num_tracks  \\\n",
       "0  7u1RhzyK1ykPQ1wfemcqoR          False  Low viscosity vibes          58   \n",
       "1  0ZcZqlDEw4eLILfHdni8vG          False            dalanda üêâ         104   \n",
       "2  0Z48DgMwRPZVVTF7Y8RLB6          False          freeze pops          52   \n",
       "3  1G8V8nWYp6uVacS8XAPSbo          False        Golden Oldies          98   \n",
       "\n",
       "   num_followers  num_artists  num_albums  duration_ms  modified_at  \\\n",
       "0              3           25          38     15191903   1559722465   \n",
       "1              4           92         100     22361466   1558851313   \n",
       "2              2           13          25     10643802   1552263755   \n",
       "3              2           64          90     25418594   1556948665   \n",
       "\n",
       "   num_edits          file_name  \n",
       "0         14  pls-set_07672.zip  \n",
       "1         75  pls-set_05170.zip  \n",
       "2          7  pls-set_06675.zip  \n",
       "3         34  pls-set_01145.zip  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mpd_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Eliminamos las listas que pertenecen al conjunto de prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mpd_info = df_mpd_info[0:1000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Como vimos en la [libreta](00-EstudioMPD.ipynb) donde estudiamos el fragmento del *Million Playlist Dataset*, el conjunto de datos est√° separado en 1.000 ficheros donde cada fichero contiene 1.000 playlist. En nuestro caso, lo vamos a dividir del mismo modo.\n",
    "\n",
    "Las playlists ser√°n almacenadas en ficheros siguiendo el orden en el que aparecen en el DataFrame `df_mpd_info`. Para saber a qu√© bloque/fichero pertenece cada playlist, lo haremos de la siguiente manera:\n",
    "\n",
    "\\begin{equation*}\n",
    "{bloque_n} = techo(\\frac{playlist_{posicion}}{num\\: playlists_{fichero}})  \n",
    "\\end{equation*}\n",
    "\n",
    "A continuaci√≥n, vamos a crear un diccionario donde almacenaremos a qu√© bloque pertenece cada conjunto. Este diccionario tendr√° como clave el _id_ de la playlist y su valor ser√° el bloque donde ser√° almacenada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa0bbfcc0dd4d719935d8b7545fca30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "location_tl_dict = dict()\n",
    "\n",
    "with tqdm_nb(total=len(df_mpd_info)) as pbar:\n",
    "    for pid, pl in df_mpd_info.iterrows():\n",
    "        location_tl_dict[pl['id']] = math.floor(pid/NUM_LISTS_FILE)\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Ejemplo___:\n",
    "\n",
    "Si tomamos como referencia la lista cuyo _id_ es `0GtTxbghpueJjaRxS6GOqp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>collaborative</th>\n",
       "      <th>name</th>\n",
       "      <th>num_tracks</th>\n",
       "      <th>num_followers</th>\n",
       "      <th>num_artists</th>\n",
       "      <th>num_albums</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>modified_at</th>\n",
       "      <th>num_edits</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>652386</th>\n",
       "      <td>0GtTxbghpueJjaRxS6GOqp</td>\n",
       "      <td>False</td>\n",
       "      <td>Love Songs 1970's-1990's</td>\n",
       "      <td>204</td>\n",
       "      <td>2</td>\n",
       "      <td>158</td>\n",
       "      <td>189</td>\n",
       "      <td>48753425</td>\n",
       "      <td>1552521803</td>\n",
       "      <td>24</td>\n",
       "      <td>pls-set_06216.zip</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id  collaborative                      name  \\\n",
       "652386  0GtTxbghpueJjaRxS6GOqp          False  Love Songs 1970's-1990's   \n",
       "\n",
       "        num_tracks  num_followers  num_artists  num_albums  duration_ms  \\\n",
       "652386         204              2          158         189     48753425   \n",
       "\n",
       "        modified_at  num_edits          file_name  \n",
       "652386   1552521803         24  pls-set_06216.zip  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mpd_info[df_mpd_info.id == '0GtTxbghpueJjaRxS6GOqp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Vemos que est√° en la posici√≥n _652386_ del DataFrame `df_mpd_info`. Con este dato, deducimos que esta lista pertenece al bloque que recoge las playlists que hay entre las posiciones 652000 y 652999. Con lo que el n√∫mero del bloque al que pertenece es al _652_.\n",
    "\n",
    "Si ahora vamos al diccionario y vemos el valor que contiene la clave `0GtTxbghpueJjaRxS6GOqp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "652"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_tl_dict['0GtTxbghpueJjaRxS6GOqp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que efectivamente se corresponde con el bloque al que pertenece dicha lista.\n",
    "\n",
    "<br>\n",
    "\n",
    "Antes de comenzar a generar nuestro DataSet, nos faltar√≠a reunir las listas de pistas que pertenecen a cada playlist. Como estas listas est√°n dispersas entre m√°s de 10.000 ficheros, puesto que cuando descargamos las playlists se guardaban respecto al fichero de identificadores que la conten√≠an, vamos a agruparlas seg√∫n al bloque al que pertenece su playlist (diccionario `location_tl_dict`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n mediante la cual se almacena un conjunto de listas de\n",
    "# reproducci√≥n en base al bloque al que pertenecen dentro de nuestro\n",
    "# DataSet\n",
    "def store_tls_dump(set_id,tls,tls_sets_path):\n",
    "    \"\"\"\n",
    "    :param set_id: N√∫mero de bloque al que pertenecen las listas a almacenar.\n",
    "    :param tls: Conjunto de listas de pistas que queremos almacenar.\n",
    "    :param tls_sets_path: Directorio donde almacenaremos los conjuntos.\n",
    "    \"\"\"\n",
    "    # Variable que emplearemos para identificar el volcado\n",
    "    dump_num = 0\n",
    "    \n",
    "    # Contamos los ficheros existentes para los volcados de datos del conjunto \n",
    "    # que vamos a almacenar. Lo emplearemos para el identificador del volcado del bloque\n",
    "    for file_name in os.listdir(tls_sets_path):\n",
    "        if file_name.startswith('track-lists-set_{:05d}.'.format(set_id)):\n",
    "            dump_num += 1\n",
    "    \n",
    "    file_name = 'track-lists-set_{:05d}.dump{}.json'.format(set_id,dump_num)\n",
    "    file_path = os.path.join(tls_sets_path,file_name)\n",
    "       \n",
    "    # Almacenamos el nuevo conjunto de listas\n",
    "    with open(file_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(tls, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada vez que realizamos un volcado de listas de reproducci√≥n con un mismo identificador, estos datos se almacenan en un nuevo fichero. \n",
    "\n",
    "Por ejemplo, si realizamos 5 volcados de datos pertenecientes al bloque *598*, tendremos 5 ficheros diferentes. \n",
    "\n",
    "A estos volcados de datos, aparte de indicarse a qu√© bloque pertenecen, se les a√±adir√° un sufijo *.dump_NUMERO-VOLCADO* para identificar dicho volcado.\n",
    "\n",
    "Una vez que se termine el proceso de agrupaci√≥n de las listas de pistas que pertenecen a un bloque, mediante la funci√≥n ***merge_dump_sets*** nos encargaremos de unir en un s√≥lo fichero todos aquellos volcados que pertenezcan a un bloque.\n",
    "\n",
    "Este m√©todo funciona de la siguiente manera:\n",
    "\n",
    "1. Omitiendo los sufijos *'.dump_'*, identificamos a qu√© bloques pertenecen los distintos volcados.\n",
    "2. Mediante los nombres que hemos obtenido al eliminar los sufijos y que hemos almacenado en un conjunto, procedemos a identificar los volcados que pertenecen a los diferentes bloques que tenemos.\n",
    "3. Leemos todos los volcados pertenecientes a un bloque.\n",
    "4. Unimos los volcados en un √∫nico fichero.\n",
    "5. Eliminamos los volcados tras crear el fichero √∫nico.\n",
    "\n",
    "***Ejemplo***:\n",
    "\n",
    "Los ficheros:\n",
    "- *track-lists-set_109.dump0.json*\n",
    "- *track-lists-set_109.dump1.json*\n",
    "- *track-lists-set_109.dump2.json*\n",
    "- *track-lists-set_109.dump3.json*\n",
    "- *track-lists-set_109.dump4.json*\n",
    "\n",
    "Pertenecen al bloque _109_. Tras leerlos, generamos un nuevo fichero y borramos los anteriores (que conten√≠an la terminaci√≥n *.dump_*):\n",
    "- *track-lists-set_109.json*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©todo empleado para unir los volcados de datos pertenecientes a un bloque\n",
    "def merge_dump_sets(tls_sets_path):\n",
    "    \"\"\"\n",
    "    :param tls_sets_path: Directorio donde se encuentran los conjuntos de listas.\n",
    "    \"\"\"\n",
    "    # Identificamos los bloques\n",
    "    names_set = set()    \n",
    "    for file in os.listdir(tls_sets_path):\n",
    "        names_set.add(file.split('.')[0])\n",
    "        \n",
    "    # Buscamos los volcados que pertenecen a un mismo bloque\n",
    "    # y los unimos en un √∫nico fichero\n",
    "    pbar_merge = tqdm_nb(names_set)\n",
    "    pbar_merge.set_description('Uni√≥n de ficheros')\n",
    "    for name in pbar_merge:\n",
    "        # Lista que contendr√° todas las listas de pistas\n",
    "        # pertenecientes a un bloque\n",
    "        dump_tls_set = []\n",
    "        \n",
    "        # Lista que contiene los nombres de los ficheros\n",
    "        # de los volcados de un bloque para posteriormente\n",
    "        # borrarlos\n",
    "        files_to_delete = []\n",
    "        \n",
    "        # Proceso de uni√≥n de listas de pistas\n",
    "        for file in os.listdir(tls_sets_path):\n",
    "            if file.startswith(name):\n",
    "                file_path = os.path.join(tls_sets_path, file)\n",
    "                files_to_delete.append(file_path)\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8') as json_file: \n",
    "                    data = json.load(json_file)                \n",
    "                dump_tls_set += data\n",
    "\n",
    "        # Guardamos un fichero con la uni√≥n de todos los volcados\n",
    "        dump_file_name = f'{name}.json'\n",
    "        dump_file_path = os.path.join(tls_sets_path, dump_file_name)\n",
    "        with open(dump_file_path, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(dump_tls_set, json_file, indent=4)\n",
    "        \n",
    "        # Una vez se han unido los volcados, procedemos a borrarlos\n",
    "        for file_path in files_to_delete:\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Definimos tambi√©n un m√©todo que emplearemos para comprimir los ficheros _JSON_ de un directorio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprime los ficheros JSON (uno a uno) de un directorio dado\n",
    "def zip_json_files(folder_path):\n",
    "    \"\"\"\n",
    "    :param folder_path: Directorio donde se encuentran ficheros JSON a comprimir.\n",
    "    \"\"\"\n",
    "    # Comprimimos cada fichero.\n",
    "    # Una vez comprimido eliminamos el fichero JSON\n",
    "    for file in tqdm_nb(os.listdir(folder_path)):\n",
    "        if file.endswith(\".json\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            zip_file_name = os.path.splitext(file)[0] + \".zip\"\n",
    "            zip_file_path = os.path.join(folder_path, zip_file_name)\n",
    "            with zipfile.ZipFile(zip_file_path,'w') as zip_file: \n",
    "                 zip_file.write(file_path, compress_type=zipfile.ZIP_DEFLATED, arcname=file)\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Comenzamos el proceso de generar los nuevos ficheros que contendr√°n las listas de pistas que deseamos obtener para nuestro DataSet.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> __Nota__:\n",
    "Para reducir el n√∫mero de operaciones de lectura y escritura de los ficheros _JSON_ donde vamos a almacenar las listas, hemos establecido que estos datos se guardaran en los nuevos ficheros una vez se hayan le√≠do 1.250 ficheros `.zip`.\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> __Advertencia__:\n",
    "El n√∫mero de esta condici√≥n que acabamos de establecer, debe decidirse dependiendo del tama√±o de la memoria RAM del equipo donde nos encontremos trabajando, puesto que podr√≠a darse el caso de que nos quedemos sin memoria disponible para la lectura de las listas de pistas.\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tls_sets(tls_path, pls_path, max_files):\n",
    "    \"\"\"\n",
    "    :param tls_path: Directorio que contiene las listas de pistas.\n",
    "    :param pls_path: Directorio que contiene las playlists obtenidas.\n",
    "    :param max_files: N√∫mero m√°ximo de listas de pistas que contiene un fichero.\n",
    "    \"\"\"        \n",
    "    # Almacenamos los ids de aquellas playlist que tenemos en nuestro conjunto\n",
    "    ids_set = set(df_mpd_info.id.to_list())\n",
    "\n",
    "    # Diccionario que emplearemos para almacenar temporalmente\n",
    "    # las listas de pistas que leemos de los ficheros comprimidos\n",
    "    tl_set_dict = defaultdict(list)\n",
    "\n",
    "    # Contador para almacenar el n√∫mero de ficheros .zip que llevamos le√≠dos.\n",
    "    count = 0\n",
    "\n",
    "    # Si no existe el directorio donde almacenaremos los\n",
    "    # conjuntos de listas de pistas, lo creamos.\n",
    "    if not os.path.exists(tls_path):\n",
    "        os.mkdir(tls_path)\n",
    "        \n",
    "    last_file_name = os.listdir(pls_path)[-1]\n",
    "\n",
    "    # Barra de progreso para el almacenamiento de las listas de pistas\n",
    "    pbar_tls = tqdm_nb(total=NUM_LISTS_FILE, position=1)\n",
    "    pbar_tls.set_description(f'Almacenamiento de datos')\n",
    "    \n",
    "    # Leemos cada fichero comprimido que se encuentre en el directorio \n",
    "    # donde est√°n almacenadas las playlists y cargamos los datos que contiene\n",
    "    pbar_read = tqdm_nb(os.listdir(pls_path))\n",
    "    pbar_read.set_description('Lectura de ficheros')\n",
    "    for file_name in pbar_read:\n",
    "        file_path = os.path.join(pls_path,file_name)\n",
    "        with zipfile.ZipFile(file_path,'r') as zip_file:\n",
    "            with zip_file.open(zip_file.namelist()[0]) as json_file:\n",
    "                data = json_file.read()\n",
    "                list_of_pls = json.loads(data.decode())\n",
    "\n",
    "        # Para cada playlist que hemos le√≠do del fichero, comprobamos que est√°n\n",
    "        # en el conjunto de ids que aquellas listas que nos interesan y \n",
    "        # almacenamos la lista de pistas correspondiente\n",
    "        for pl in list_of_pls:\n",
    "            if pl['id'] in ids_set:\n",
    "                tl_set_dict[location_tl_dict[pl['id']]].append({'id' : pl['id'], 'tracks' : pl['tracks']})\n",
    "                ids_set.remove(pl['id'])\n",
    "\n",
    "        # Aumentamos el contador de ficheros le√≠dos\n",
    "        count += 1\n",
    "\n",
    "        # Comprobamos que hemos le√≠do el n√∫mero de ficheros establecido, o que \n",
    "        # es el √∫ltimo fichero que se va a tratar, para guardar los datos\n",
    "        # en el nuevo conjunto de ficheros .json\n",
    "        if (count == max_files) or (file_name == last_file_name):\n",
    "            # Reiniciamos la barra de progreso del proceso de escritura\n",
    "            pbar_tls.reset()\n",
    "            \n",
    "            for set_id, tls in tl_set_dict.items():\n",
    "                store_tls_dump(set_id,tls,tls_path)\n",
    "                pbar_tls.update()\n",
    "            \n",
    "            # Borramos el contenido de la lista temporal\n",
    "            tl_set_dict = defaultdict(list)\n",
    "            \n",
    "            # Reiniciamos el contador\n",
    "            count = 0\n",
    "            \n",
    "    merge_dump_sets(tls_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tls_sets(TLS_SETS_PATH,PLS_SETS_PATH,1250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el tama√±o de los nuevos ficheros que hemos generado para los conjuntos de listas de pistas de nuestras playlist es muy elevado, vamos a comprimirlos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_json_files(TLS_SETS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Una vez terminado el proceso anterior, procedemos a crear nuestro DataSet en formato *JSON*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "## <font color=\"#92002A\">3 - Generaci√≥n del DataSet</font>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado, crearemos nuestro DataSet en formato [*JSON*](https://www.json.org/). El proceso que vamos a seguir, para cada bloque/fichero que debemos crear, es el siguiente:\n",
    "\n",
    "1. Establecemos el valor '*info*' del fichero *JSON* (cabecera con los datos pertenecientes al bloque).\n",
    "2. Leemos el fichero que contiene las listas de pistas de las playlist pertenecientes al bloque.\n",
    "3. Para cada _id_ de la lista de reproducci√≥n que pertenece al bloque, cargamos sus datos junto a la lista de pistas y los guardamos en el valor '*playlist*' del fichero *JSON* a crear.\n",
    "4. Generamos el fichero para posteriormente comprimirlo y quedarnos √∫nicamente con el fichero `.zip`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> __Nota__:\n",
    "Vamos a aprovechar el valor de la columna _index_ del DataFrame `df_mpd_info` para crear una nueva variable llamada ___pid___, que nos servir√° para identificar una playlist dentro de nuestro conjunto (no confundir con *id*, que es el identificador de la lista en *Spotify*).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mpd_json(mpd_path, tls_sets_path):\n",
    "    \"\"\"\n",
    "    :param mpd_path: Directorio donde se almacenar√° el dataset en formato JSON.\n",
    "    :param tls_sets_path: Directorio que contiene las playlists obtenidas.\n",
    "    \"\"\"    \n",
    "    # Si no existe el directorio donde vamos a almacenar\n",
    "    # los ficheros pertenecientes a nuestro DataSet, lo creamos.\n",
    "    if not os.path.exists(mpd_path):\n",
    "        os.makedirs(mpd_path)\n",
    "        \n",
    "    # Establecemos el n√∫mero de elementos que contendr√° cada fichero.\n",
    "    chunk_size = NUM_LISTS_FILE\n",
    "\n",
    "    # Contador donde indicamos el bloque actual.\n",
    "    chunk_num = 0\n",
    "\n",
    "    # N√∫mero de ficheros a generar.\n",
    "    chunk_total = int(len(df_mpd_info) / chunk_size)\n",
    "\n",
    "    # Creamos el esqueleto de la cabecera de cada bloque\n",
    "    # (clave 'info' del diccionario).\n",
    "    dic_info = dict()\n",
    "    \n",
    "    dic_info = {'generated_on' : str(datetime.datetime.now()),\n",
    "                'slice' : '', 'version': 'v1.1'}\n",
    "\n",
    "    # Creamos la barra para seguir el progreso de escritura \n",
    "    # de cada fichero.\n",
    "    pbar_file = tqdm_nb(total=chunk_size, position=1)\n",
    "\n",
    "    with tqdm_nb(total=chunk_total, position=0) as pbar_general:\n",
    "        pbar_general.set_description('MPD')\n",
    "        while chunk_num < chunk_total:\n",
    "            pbar_file.reset()\n",
    "            pbar_file.set_description(f'Slice {chunk_num + 1}')\n",
    "\n",
    "            # Establecemos el conjunto de filas del DataFrame que\n",
    "            # vamos a guardar en el bloque.\n",
    "            start_pid = chunk_num * chunk_size\n",
    "            end_pid = (chunk_num + 1) * chunk_size\n",
    "\n",
    "            # Creamos el diccionario correspondiente al fichero\n",
    "            # JSON que vamos a guardar y establecemos la cabecera.\n",
    "            file = dict()        \n",
    "            file['info'] = dic_info\n",
    "            file['info']['slice'] = \"{:06d}-{:06d}\".format(start_pid, end_pid - 1)\n",
    "            file['playlists'] = list()        \n",
    "            file['playlists'] = []\n",
    "\n",
    "            # Leemos el conjunto de listas de pistas que pertenecen \n",
    "            # a las playlists del conjunto.\n",
    "            tls_file_name = 'track-lists-set_{:05d}.zip'.format(chunk_num)\n",
    "            tls_file_path = os.path.join(tls_sets_path,tls_file_name)\n",
    "            \n",
    "            with zipfile.ZipFile(tls_file_path,'r') as zip_file:\n",
    "                with zip_file.open(zip_file.namelist()[0]) as json_file:\n",
    "                    data = json_file.read()\n",
    "            list_of_tls = json.loads(data.decode())\n",
    "\n",
    "            # Guardamos el contenido completo de cada playlist en \n",
    "            # un diccionario\n",
    "            for pid, pl in df_mpd_info[start_pid:end_pid].iterrows():\n",
    "                pl_dict_item = dict()\n",
    "\n",
    "                pl_dict_item['pid'] = pid\n",
    "                pl_dict_item['name'] = pl['name']\n",
    "                pl_dict_item['collaborative'] = pl['collaborative']\n",
    "                pl_dict_item['modified_at'] = pl['modified_at']\n",
    "                pl_dict_item['num_albums'] = pl['num_albums']\n",
    "                pl_dict_item['num_tracks'] = pl['num_tracks']\n",
    "                pl_dict_item['num_followers'] = pl['num_followers']\n",
    "                pl_dict_item['num_edits'] = pl['num_edits']\n",
    "                pl_dict_item['duration_ms'] = pl['duration_ms']\n",
    "                pl_dict_item['num_artists'] = pl['num_artists']\n",
    "                pl_dict_item['tracks'] = next(tl['tracks'] for tl in list_of_tls if tl['id'] == pl['id'])\n",
    "                for track in pl_dict_item['tracks']: del track['added_at']\n",
    "\n",
    "                file['playlists'].append(pl_dict_item)\n",
    "                pbar_file.update()\n",
    "\n",
    "            # Creamos el fichero .json\n",
    "            file_name = \"mpd.slice.{:06d}-{:06d}.json\".format(int(start_pid),int(end_pid - 1))\n",
    "            file_path = os.path.join(mpd_path, file_name)\n",
    "            with open(file_path, 'w') as f:\n",
    "                json.dump(file,f,indent=4)\n",
    "\n",
    "            # Comprimimos el fichero que hemos creado\n",
    "            zip_file_name = file_name.replace('.json', '.zip')\n",
    "            zip_file_path = os.path.join(mpd_path, zip_file_name)\n",
    "            with zipfile.ZipFile(zip_file_path,'w') as zip_file: \n",
    "                 zip_file.write(file_path, compress_type=zipfile.ZIP_DEFLATED, arcname=file_name)\n",
    "\n",
    "            # Eliminamos el fichero creado para quedarnos con\n",
    "            # la versi√≥n comprimida\n",
    "            os.remove(file_path)\n",
    "            \n",
    "            # Incrementamos el n√∫mero de bloque\n",
    "            chunk_num +=1\n",
    "            pbar_general.update()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_mpd_json(MPD_PATH, TLS_SETS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Una vez creado el DataSet en formato *JSON*, por seguridad, vamos a comprobar que el n√∫mero de pistas almacenado en la variable '*num_tracks*' coincide con el tama√±o de la lista de pistas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_num_tracks(mpd_path):\n",
    "    '''\n",
    "    :param mpd_path: Directorio donde se encuentra el dataset en formato JSON.\n",
    "    :return: Devuelve True/False si el 'num_tracks' coincide con el tama√±o \n",
    "             de las listas de pistas\n",
    "    '''\n",
    "    # Variable que emplearemos para saber si en todas\n",
    "    # las playlist coincide el valor de 'num_tracks'\n",
    "    # con el tama√±o de la lista de pistas ('tracks')\n",
    "    same_values = True\n",
    "\n",
    "    for file in tqdm_nb(os.listdir(mpd_path)):\n",
    "        zip_file_path = os.path.join(mpd_path, file)\n",
    "        with zipfile.ZipFile(zip_file_path,'r') as zip_file:\n",
    "            with zip_file.open(zip_file.namelist()[0]) as json_file:\n",
    "                data = json_file.read()\n",
    "                slice_data = json.loads(data.decode())\n",
    "\n",
    "        playlists = slice_data['playlists']\n",
    "\n",
    "        for pl in playlists:\n",
    "            if pl['num_tracks'] != len(pl['tracks']):\n",
    "                same_values = False\n",
    "                break\n",
    "\n",
    "        if not same_values:\n",
    "            break\n",
    "            \n",
    "    return same_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ec7507e550461bb9af96a1896dd5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_num_tracks(MPD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Vemos que la variable *num_track* coincide con el tama√±o de la lista de pistas (*tracks*) para todas las playlists del conjunto.\n",
    "\n",
    "Para finalizar, vamos a eliminar del directorio `DATA_PATH` aquellos ficheros que ya no nos resultar√°n de utilidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(os.path.join(DATA_PATH, 'downloaded_pls.csv'))\n",
    "os.remove(os.path.join(DATA_PATH, 'downloaded_pls_artists.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-graduation-cap\" aria-hidden=\"true\" style=\"color:#92002A\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
