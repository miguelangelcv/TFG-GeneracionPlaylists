{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/header.png\" alt=\"Logo UCLM-ESII\" align=\"right\">\n",
    "\n",
    "<br><br><br><br>\n",
    "<h2><font color=\"#92002A\" size=4>Trabajo Fin de Grado</font></h2>\n",
    "\n",
    "<h1><font color=\"#6B001F\" size=5>Generación automática de playlist de canciones <br> mediante técnicas de minería de datos</font></h1>\n",
    "<h2><font color=\"#92002A\" size=3>Parte 4 - Filtrado de playlists</font></h2>\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: right\">\n",
    "    <font color=\"#B20033\" size=3><strong>Autor</strong>: <em>Miguel Ángel Cantero Víllora</em></font><br>\n",
    "    <br>\n",
    "    <font color=\"#B20033\" size=3><strong>Directores</strong>: <em>José Antonio Gámez Martín</em></font><br>\n",
    "    <font color=\"#B20033\" size=3><em>Juan Ángel Aledo Sánchez</em></font><br>\n",
    "    <br>\n",
    "<font color=\"#B20033\" size=3>Grado en Ingeniería Informática</font><br>\n",
    "<font color=\"#B20033\" size=2>Escuela Superior de Ingeniería Informática | Universidad de Castilla-La Mancha</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br>\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#92002A\" size=5>Índice</font></h2>\n",
    "\n",
    "<br>\n",
    "\n",
    "* [1. Introducción](#section1)\n",
    "* [2. Carga de resultados](#section2)\n",
    "* [3. Comprobación de valores perdidos](#section3)\n",
    "* [4. Filtrado de playlists](#section4)\n",
    "    * [4.1. Filtrado por número de pistas, artistas y álbumes](#section41)\n",
    "    * [4.2. Filtrado de playlists con idénticas características](#section42)\n",
    "    * [4.3. Filtrado por número de ediciones](#section43)\n",
    "    * [4.4. Filtrado por títulos](#section44)\n",
    "        * [Número de caracteres](#section441)\n",
    "        * [Número de palabras](#section442)\n",
    "        * [Existencia de emoticonos](#section443)\n",
    "        * [Alfabeto](#section444)\n",
    "        * [Idioma](#section445)\n",
    "        * [Títulos ofensivos](#section446)\n",
    "    * [4.5. Consideraciones en el filtrado de playlists](#section45)\n",
    "* [5. Almacenamiento de resultados](#section5)\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Permite establecer la anchura de la celda\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import emoji\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.parse\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from emoji import UNICODE_EMOJI\n",
    "from shutil import copyfile\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id=\"section1\"></a>\n",
    "## <font color=\"#92002A\">1 - Introducción</font>\n",
    "<br>\n",
    "\n",
    "Una vez que ya hemos descargado aquellas listas de reproducción que nos interesaba estudiar con más detalle, vamos a realizar el proceso completo de filtrado. Nuestro objetivo es quedarnos con _1.000.000 de playlists_. También aprovecharemos este filtrado para obtener un conjunto de prueba.\n",
    "\n",
    "Recordemos aquellos criterios que ya hemos aplicado en anteriores libretas para descartar listas de reproducción:\n",
    "\n",
    "1. Eliminación de playlists con identificadores repetidos (se ha obtenido la misma lista tras buscar con diferentes términos).<sup>1</sup>\n",
    "2. Tienen menos de 5 pistas o más de 250 pistas.<sup>1</sup>\n",
    "3. El título contiene menos de 2 caracteres o más de 50 caracteres (sin contar espacios).<sup>1</sup>\n",
    "4. El título contiene más de 9 palabras.<sup>1</sup>\n",
    "5. No tienen ningún seguidor.<sup>2</sup>\n",
    "6. Están marcadas como privadas.<sup>2</sup>\n",
    "7. Contienen pistas locales.<sup>2</sup>\n",
    "\n",
    "En esta libreta también volveremos a aplicar los criterios 2, 3 y 4, puesto que durante el proceso de búsqueda y el proceso de descarga han transcurrido varios días y el usuario podría haber modificado el título o las pistas que pertenecen a la playlist.\n",
    "\n",
    "Por último, en caso de que no hayamos conseguido reducir el número de playlists, añadiremos dos últimos criterios:\n",
    "- Playlists que contengan artistas poco frecuentes (sólo aparecen en 1 ó 2 listas).\n",
    "- Número de seguidores inferior a 2.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<sup>1</sup>\n",
    "<font size=\"2\"> _Ver libreta_ ['Preparación de datos para el proceso de descarga'](02-PreparacionDescarga.ipynb).</font>\n",
    "\n",
    "<sup>2</sup>\n",
    "<font size=\"2\"> _Ver libreta_ ['Descarga de playlists'](03-DescargaPlaylists.ipynb).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "## <font color=\"#92002A\">2 - Carga de resultados</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "Antes de comenzar con el estudio de las playlist obtenidas en el proceso de descarga, definimos aquellos directorios con los que vamos a trabajar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales\n",
    "\n",
    "# Directorio donde se encuentran almacenados los conjuntos \n",
    "# de playlists que hemos descargado\n",
    "PLS_SETS_PATH = 'pls_sets'\n",
    "\n",
    "# Directorio empleado para guardar/leer aquellos datos generados\n",
    "# que resultan de interés para diferentes tareas\n",
    "DATA_PATH = 'data'\n",
    "\n",
    "# Directorio que empleamos para guardar copias de seguridad\n",
    "BACKUP_PATH = 'backup'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Para obtener la información que nos faltaba por comprobar en el proceso de limpieza de playlist, creamos la función ***get_playlist_info***. Esta función se encargara de obtener el número de artistas y álbumes diferentes que hay en una playlist junto a la duración total de ésta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera la información adicional de una playlist que nos resultará\n",
    "# útil para su filtrado\n",
    "def get_playlist_info(track_list):\n",
    "    \"\"\"\n",
    "    :param track_list: Lista de tracks a analizar.\n",
    "    :return: Información generada. Tupla (artistas,álbumes,duración).\n",
    "    \"\"\"\n",
    "    albums = set()\n",
    "    artists = set()\n",
    "    duration = 0\n",
    "    for track in track_list:\n",
    "        albums.add(track['album_uri'].split(':')[-1])\n",
    "        artists.add(track['artist_uri'].split(':')[-1])\n",
    "        duration += track['duration_ms']\n",
    "    return len(artists), len(albums), duration, list(artists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "El siguiente método nos devuelve el número de veces que se ha editado una playlist, mediante las fechas obtenidas de la variable *added_at* de cada pista.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> __Nota__:\n",
    "Se considera que las pistas agregadas en una ventana de dos horas han sido agregadas en una sola sesión de edición.\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_playlist_edits(date_list):\n",
    "    \"\"\"\n",
    "    :param date_list: Lista de fechas a estudiar.\n",
    "    :return: Número de veces que se ha editado.\n",
    "    \"\"\"\n",
    "    if len(date_list) < 1:\n",
    "        return None\n",
    "    else:\n",
    "        dates = date_list.copy()\n",
    "        dates.sort()\n",
    "        edits_count= 1\n",
    "        n_init = dates.pop(0)\n",
    "        while len(dates) > 0:\n",
    "            num = dates.pop(0)\n",
    "            # 2h -> 7200s\n",
    "            if num > n_init + 7200:\n",
    "                n_init = num\n",
    "                edits_count += 1\n",
    "        return edits_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "__Importante__: En nuestro caso, vamos a contar también como una edición la primera vez que se añadieron pistas a la playlist. Por lo que tendríamos lo siguiente:\n",
    "- Si el número de ediciones es 1, la playlist no se ha modificado tras su creación.\n",
    "- Si el número de ediciones es igual o superior a 2, la playlist se ha editado al menos una vez tras su creación.\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método ***get_playlist_date_info*** nos devuelve la información procedente del estudio de las fechas en las que fueron añadidas las pistas a la playlist:\n",
    "\n",
    "- La última fecha en la que la lista ha sido modificada (*modified_at*).\n",
    "- El número de veces que ha sido editada (*num_edits*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_playlist_date_info(track_list):\n",
    "    \"\"\"\n",
    "    :param date_list: Lista de pistas a estudiar.\n",
    "    :return: Tupla con el número de veces que se ha editado y la última fecha de modificación.\n",
    "    \"\"\"\n",
    "    if len(track_list) < 1:\n",
    "        return None, None\n",
    "    else:\n",
    "        dates = [track['added_at'] for track in track_list]\n",
    "        modified_at = max(dates)\n",
    "        num_edits = get_playlist_edits(dates)\n",
    "\n",
    "        return modified_at, num_edits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Creamos un DataFrame donde almacenaremos todas las playlists, junto a la información que queremos estudiar. Almacenaremos el resultado de este diccionario en un archivo `.csv` comprimido para tener una copia de seguridad y, como la duración del proceso es considerable, evitar volver a crear el diccionario desde 0 en caso de querer ejecutar la libreta de nuevo.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> __Nota__:\n",
    "Para evitar que el DataFrame resultante de leer todas las playlist descargadas tuviera un tamaño demasiado grande, y ya que para el filtrado que vamos a realizar no nos es necesario, se ha eliminado la lista de pistas que pertenece a una lista de reproducción. Por si nos fuese de necesidad recuperar dichos datos, añadimos el nombre del archivo `.zip` donde se encuentra la playlist.\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Mientras se realiza este proceso, también vamos a crear un archivo en el que almacenaremos qué artistas pertenecen a una playlist para posteriormente aplicar dicho filtrado (en caso de ser necesario).\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_downloaded_playlists_df(pl_sets_path):\n",
    "    # La información obtenida será almacenada en un diccionario\n",
    "    # que utilizaremos para crear el DataFrame por primera vez\n",
    "    pls_dict = {}\n",
    "    \n",
    "    csv_file_path = os.path.join(DATA_PATH,'downloaded_pls_artists.csv')\n",
    "    \n",
    "    # Para cada fichero del directorio, lo descomprimimos y lo leemos\n",
    "    # para obtener la información que nos es relevante para el filtrado\n",
    "    for file_name in tqdm_nb(os.listdir(pl_sets_path)):\n",
    "        if file_name.startswith('pls-set') and file_name.endswith('.zip'):\n",
    "            file_path = os.path.join(pl_sets_path, file_name)\n",
    "            with zipfile.ZipFile(file_path,'r') as zip_file:\n",
    "                with zip_file.open(zip_file.namelist()[0]) as json_file:\n",
    "                    data = json_file.read() \n",
    "            data = json.loads(data.decode())\n",
    "            \n",
    "            artists_dict = dict()\n",
    "            \n",
    "            for d in data:      \n",
    "                num_artists, num_albums, duration, artists_list = get_playlist_info(d['tracks'])\n",
    "                d['num_artists'] = num_artists\n",
    "                d['num_albums'] = num_albums\n",
    "                d['duration_ms'] = duration\n",
    "                artists_dict[d['id']] = \"|\".join(artists_list)\n",
    "\n",
    "                modified_at, num_edits = get_playlist_date_info(d['tracks'])\n",
    "                d['modified_at'] = modified_at\n",
    "                d['num_edits'] = num_edits\n",
    "\n",
    "                del d['tracks']\n",
    "\n",
    "                pl_id = d['id']\n",
    "                del d['id']\n",
    "                \n",
    "                d['file_name'] = file_name\n",
    "\n",
    "                pls_dict[pl_id] = d\n",
    "                \n",
    "            with open(csv_file_path, 'a+', newline='') as artists_file:\n",
    "                csv_writer = csv.writer(artists_file, delimiter=';', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "                for key,value in artists_dict.items():\n",
    "                    csv_writer.writerow([key, value])\n",
    "    \n",
    "    # Creamos el DataFrame a partir del diccionario generado\n",
    "    df_downloaded_pls = pd.DataFrame.from_dict(pls_dict, orient='index')\n",
    "    df_downloaded_pls.index.name = 'id'\n",
    "    \n",
    "    return df_downloaded_pls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de playlists obtenidas: 3122640\n"
     ]
    }
   ],
   "source": [
    "# Nombre del DataSet comprimido donde se encuentra almacenada la \n",
    "# información de las playlists tras generarse el DataFrame\n",
    "# por primera vez\n",
    "playlists_file = os.path.join(DATA_PATH,'downloaded_pls.csv')\n",
    "playlists_artists_file = os.path.join(DATA_PATH,'downloaded_pls_artists.csv')\n",
    "\n",
    "# En caso de que el fichero exista, lo cargamos para evitar \n",
    "# repetir el proceso de creación\n",
    "if os.path.isfile(playlists_file) and os.path.isfile(playlists_artists_file):\n",
    "    df_playlists_info = pd.read_csv(playlists_file, sep=';',\n",
    "                                    encoding='utf-8', index_col=0)\n",
    "else:\n",
    "    # Si no existe la carpeta 'DATA_PATH', la creamos\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        os.makedirs(DATA_PATH)\n",
    "    df_playlists_info = get_downloaded_playlists_df(PLS_SETS_PATH)\n",
    "    \n",
    "    # Volcamos el contenido del DataFrame a un fichero .csv\n",
    "    csv_file_path = os.path.join(DATA_PATH,'downloaded_pls.csv')\n",
    "    df_playlists_info.to_csv(csv_file_path, sep=';', encoding='utf-8')\n",
    "    \n",
    "    # Hacemos una copia de seguridad del archivo .csv\n",
    "    copyfile(csv_file_path, os.path.join(BACKUP_PATH, 'downloaded_pls.csv'))\n",
    "\n",
    "print(f\"Número de playlists obtenidas: {len(df_playlists_info)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Para comprobar que el DataFrame ha sido creado de forma correcta, mostramos las primeras 5 filas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collaborative</th>\n",
       "      <th>name</th>\n",
       "      <th>num_tracks</th>\n",
       "      <th>num_followers</th>\n",
       "      <th>num_artists</th>\n",
       "      <th>num_albums</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>modified_at</th>\n",
       "      <th>num_edits</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3FI0aZZz9hUw5qK0K2GDwH</th>\n",
       "      <td>False</td>\n",
       "      <td>Love Letter</td>\n",
       "      <td>17</td>\n",
       "      <td>65.0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>3601989</td>\n",
       "      <td>1.556283e+09</td>\n",
       "      <td>5.0</td>\n",
       "      <td>pls-set_00000.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381M0rlWt8mB0CayapCcRr</th>\n",
       "      <td>False</td>\n",
       "      <td>skin deep</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>7146193</td>\n",
       "      <td>1.558387e+09</td>\n",
       "      <td>16.0</td>\n",
       "      <td>pls-set_00000.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3nSWAjR9QuXhb3YqO24HHw</th>\n",
       "      <td>False</td>\n",
       "      <td>junior year</td>\n",
       "      <td>250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>113</td>\n",
       "      <td>184</td>\n",
       "      <td>57370653</td>\n",
       "      <td>1.556406e+09</td>\n",
       "      <td>113.0</td>\n",
       "      <td>pls-set_00000.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5M6FDo7oQ33CvNMaMiZ1uH</th>\n",
       "      <td>False</td>\n",
       "      <td>BOOM BOOM</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>4120665</td>\n",
       "      <td>1.555591e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>pls-set_00000.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43QPMRPsHlbXILXIhYoxF6</th>\n",
       "      <td>False</td>\n",
       "      <td>Born in the Wrong Era</td>\n",
       "      <td>168</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112</td>\n",
       "      <td>146</td>\n",
       "      <td>36941201</td>\n",
       "      <td>1.559002e+09</td>\n",
       "      <td>86.0</td>\n",
       "      <td>pls-set_00000.zip</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        collaborative                   name  num_tracks  \\\n",
       "id                                                                         \n",
       "3FI0aZZz9hUw5qK0K2GDwH          False            Love Letter          17   \n",
       "381M0rlWt8mB0CayapCcRr          False              skin deep          35   \n",
       "3nSWAjR9QuXhb3YqO24HHw          False           junior year          250   \n",
       "5M6FDo7oQ33CvNMaMiZ1uH          False              BOOM BOOM          20   \n",
       "43QPMRPsHlbXILXIhYoxF6          False  Born in the Wrong Era         168   \n",
       "\n",
       "                        num_followers  num_artists  num_albums  duration_ms  \\\n",
       "id                                                                            \n",
       "3FI0aZZz9hUw5qK0K2GDwH           65.0           16          16      3601989   \n",
       "381M0rlWt8mB0CayapCcRr            1.0           32          35      7146193   \n",
       "3nSWAjR9QuXhb3YqO24HHw            1.0          113         184     57370653   \n",
       "5M6FDo7oQ33CvNMaMiZ1uH            1.0           16          20      4120665   \n",
       "43QPMRPsHlbXILXIhYoxF6            1.0          112         146     36941201   \n",
       "\n",
       "                         modified_at  num_edits          file_name  \n",
       "id                                                                  \n",
       "3FI0aZZz9hUw5qK0K2GDwH  1.556283e+09        5.0  pls-set_00000.zip  \n",
       "381M0rlWt8mB0CayapCcRr  1.558387e+09       16.0  pls-set_00000.zip  \n",
       "3nSWAjR9QuXhb3YqO24HHw  1.556406e+09      113.0  pls-set_00000.zip  \n",
       "5M6FDo7oQ33CvNMaMiZ1uH  1.555591e+09        2.0  pls-set_00000.zip  \n",
       "43QPMRPsHlbXILXIhYoxF6  1.559002e+09       86.0  pls-set_00000.zip  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_playlists_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "## <font color=\"#92002A\">3 - Comprobación de valores perdidos</font>\n",
    "<br>\n",
    "\n",
    "Antes de comenzar a filtrar los resultados, vamos a comprobar que no hay valores perdidos (**NaN**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collaborative      0\n",
       "name               7\n",
       "num_tracks         0\n",
       "num_followers    255\n",
       "num_artists        0\n",
       "num_albums         0\n",
       "duration_ms        0\n",
       "modified_at      724\n",
       "num_edits        724\n",
       "file_name          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_playlists_info.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que existen valores perdidos en las siguientes variables:\n",
    "\n",
    "* *num_followers*\n",
    "* *modified_at*\n",
    "* *num_edits*\n",
    "\n",
    "En el caso del número de seguidores, vamos a considerar que un valor **NaN** equivale a que la lista de reproducción no tiene ningún seguidor (*num_followers = 0*). \n",
    "\n",
    "Para las dos variables restantes: si no existe fecha de última modificación ni número de veces que se ha editado una playlist, la descartaremos por no poder calcular dichos valores.\n",
    "\n",
    "Por lo tanto, podemos descartar aquellas filas que contengan un valor perdido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_playlists_info.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para asegurarnos de que se han eliminado correctamente, vamos a volver a comprobar los valores perdidos del DataFrame `df_playlists_info`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collaborative    0\n",
       "name             0\n",
       "num_tracks       0\n",
       "num_followers    0\n",
       "num_artists      0\n",
       "num_albums       0\n",
       "duration_ms      0\n",
       "modified_at      0\n",
       "num_edits        0\n",
       "file_name        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_playlists_info.isna().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos apreciar, nuestro DataFrame ya no posee ningún valor **NaN**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "## <font color=\"#92002A\">4 - Filtrado de playlists</font>\n",
    "<br>\n",
    "\n",
    "Una vez que hemos recopilado toda la información que nos va a hacer falta para filtrar las playlists y hemos eliminado los valores perdidos del DataFrame, comenzamos el proceso de filtrado.\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id=\"section41\"></a>\n",
    "### <font color=\"#B20033\">4.1 - Filtrado por número de pistas, artistas y álbumes</font>\n",
    "<br>\n",
    "\n",
    "Deben de cumplirse las siguientes condiciones:\n",
    "1. El número de pistas de una playlist estará comprendido entre 5 y 250.\n",
    "2. La playlist tendrá, al menos, 3 artistas diferentes.\n",
    "3. La playlist tendrá, al menos, 2 álbumes diferentes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño df_playlists_info (con filtrado de pistas): 3114589\n"
     ]
    }
   ],
   "source": [
    "df_playlists_info.drop(df_playlists_info[df_playlists_info['num_tracks'] > 250].index,inplace=True)\n",
    "df_playlists_info.drop(df_playlists_info[df_playlists_info['num_tracks'] < 5].index,inplace=True)\n",
    "print(f\"Tamaño df_playlists_info (con filtrado de pistas): {len(df_playlists_info)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño df_playlists_info (con filtrado de artistas): 2954967\n"
     ]
    }
   ],
   "source": [
    "df_playlists_info.drop(df_playlists_info[df_playlists_info['num_artists'] < 3].index,inplace=True)\n",
    "print(f\"Tamaño df_playlists_info (con filtrado de artistas): {len(df_playlists_info)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño df_playlists_info (con filtrado de álbumes): 2954951\n"
     ]
    }
   ],
   "source": [
    "df_playlists_info.drop(df_playlists_info[df_playlists_info['num_albums'] < 2].index,inplace=True)\n",
    "print(f\"Tamaño df_playlists_info (con filtrado de álbumes): {len(df_playlists_info)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section42\"></a>\n",
    "### <font color=\"#B20033\">4.2 - Filtrado de playlists con idénticas características</font>\n",
    "<br>\n",
    "\n",
    "Hay casos en los que existen listas de reproducción iguales, pero con diferente título y pertenecientes a usuarios distintos. Para dichos casos, vamos a quedarnos con el primer resultado y borrar el resto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño df_playlists_info (con filtrado de duplicados): 2951179\n"
     ]
    }
   ],
   "source": [
    "df_playlists_info.drop_duplicates(subset=['duration_ms','num_tracks',\n",
    "                                          'num_artists','num_albums'], \n",
    "                                  inplace=True, keep='first')\n",
    "print(f\"Tamaño df_playlists_info (con filtrado de duplicados): {len(df_playlists_info)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "__Importante__: Que distintas playlists tengan valores idénticos para las siguientes variables:\n",
    "- Duración de la playlist.\n",
    "- Número de canciones.\n",
    "- Número de artistas diferentes.\n",
    "- Número de álbumes diferentes.\n",
    "\n",
    "implicaría, con una probabilidad bastante alta, de que sea la misma playlist pero con título diferente. Aunque pueden darse casos extremos en los que, coincidiendo esos datos, pueda tratarse de otra lista de reproducción distinta. En nuestro caso vamos a considerar que cuando coincidan dichos valores, se trata de la misma playlist.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section43\"></a>\n",
    "### <font color=\"#B20033\">4.3 - Filtrado por número de ediciones</font>\n",
    "<br>\n",
    "\n",
    "Para nuestro conjunto de playlists, vamos a quedarnos con aquellas que han sido editadas al menos 1 vez tras su creación:\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño df_playlists_info (con filtrado por número de ediciones): 2768027\n"
     ]
    }
   ],
   "source": [
    "df_playlists_info.drop(df_playlists_info[df_playlists_info['num_edits'] < 2].index,inplace=True)\n",
    "print(f\"Tamaño df_playlists_info (con filtrado por número de ediciones): {len(df_playlists_info)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section44\"></a>\n",
    "### <font color=\"#B20033\">4.4 - Filtrado por títulos</font>\n",
    "<br>\n",
    "\n",
    "Al igual que hicimos en la libreta [_Preparación de datos para el proceso de descarga_](02-PreparacionDescarga.ipynb), vamos a quedarnos con aquellas listas de reproducción cuyos títulos cumplan las siguientes condiciones:\n",
    "\n",
    "- Tiene entre 5 y 50 caracteres (eliminando espacios en blanco).\n",
    "- Tienen menos de 10 palabras.\n",
    "- En caso de que el título contenga emoticonos:\n",
    "    - Si el título también contiene texto, el número máximo de emoticonos es 10.\n",
    "    - Sí el título sólo contiene emoticonos, el número máximo será de 4.\n",
    "- Los caracteres del título pertenecen al alfabeto latino, al conjunto de caracteres comunes o son emoticonos.\n",
    "- Filtrado de títulos por idioma.\n",
    "- Filtrado de títulos ofensivos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section441\"></a>\n",
    "#### <font color=\"#B20033\">Número de caracteres</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño df_playlists_info (con filtrado de longitud de títulos): 2767447\n"
     ]
    }
   ],
   "source": [
    "large_names_ids = df_playlists_info[(df_playlists_info.name.str.len()-df_playlists_info.name.str.count(' ')) > 50].index\n",
    "sort_names_ids = df_playlists_info[(df_playlists_info.name.str.len()-df_playlists_info.name.str.count(' ')) < 2].index\n",
    "df_playlists_info.drop(large_names_ids,inplace=True)\n",
    "df_playlists_info.drop(sort_names_ids,inplace=True)\n",
    "print(f\"Tamaño df_playlists_info (con filtrado de longitud de títulos): {len(df_playlists_info)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"section442\"></a>\n",
    "#### <font color=\"#B20033\">Número de palabras</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño df_playlists_info (con filtrado por número de palabras en título): 2751987\n"
     ]
    }
   ],
   "source": [
    "large_names_ids = df_playlists_info[df_playlists_info.name.str.count(' ') > 10].index\n",
    "df_playlists_info.drop(large_names_ids,inplace=True)\n",
    "print(f\"Tamaño df_playlists_info (con filtrado por número de palabras en título): {len(df_playlists_info)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> __Nota__:\n",
    "Los siguientes criterios que vamos a aplicar para filtrar las playlists mediante los títulos, podrían haberse llevado a cabo en el proceso de preparación de descarga (ver libreta [*Preparación para la descarga de playlists*](02-PreparacionDescarga.ipynb)). Para evitar tanta duplicidad a la hora de realizar el filtrado de playlists, y puesto que no aportaba mucha mejora a la hora de reducir el número de listas a descargar, se decidió no aplicar estos criterios en dicho proceso.\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"section443\"></a>\n",
    "#### <font color=\"#B20033\">Existencia de emoticonos</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprueba si el carácter es un emoticono\n",
    "def is_emoji(s):\n",
    "    return s in UNICODE_EMOJI\n",
    "\n",
    "# Comprueba si un texto contiene emoticonos\n",
    "def contains_emoji(text):\n",
    "    return len([c for c in UNICODE_EMOJI if(c in str(text))]) > 0\n",
    "\n",
    "# Comprueba si un texto contiene sólo emoticonos\n",
    "def all_text_emoji(text):\n",
    "    return all([(c in UNICODE_EMOJI) for c in str(text)])\n",
    "\n",
    "# Cuenta los emoticonos que contiene un texto\n",
    "def count_text_emoji(text):\n",
    "    return sum([(c in UNICODE_EMOJI) for c in str(text)])\n",
    "\n",
    "# Elimina los emoticonos de un texto\n",
    "def remove_emoji(text):\n",
    "    return emoji.get_emoji_regexp().sub(u'', str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indica si el texto con emoticonos es válido según lo establecido\n",
    "def invalid_name_emojis(name):\n",
    "    if (all_text_emoji(name) and len(name) > 4):\n",
    "        return True\n",
    "    else:\n",
    "        return count_text_emoji(name) > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño df_playlists_info (con filtrado de títulos con emoticonos): 2751525\n"
     ]
    }
   ],
   "source": [
    "df_playlists_info.drop(df_playlists_info[df_playlists_info['name'].apply(invalid_name_emojis)].index, inplace=True)\n",
    "print(f\"Tamaño df_playlists_info (con filtrado de títulos con emoticonos): {len(df_playlists_info)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<a id=\"section444\"></a>\n",
    "#### <font color=\"#B20033\">Alfabeto</font> <br>\n",
    "\n",
    "Cuando estudiamos el [_Million Playlist Dataset_](00-EstudioMPD.ipynb), vimos que los caracteres pertenecían al alfabeto latino, al conjunto de caracteres comunes y/o eran emoticonos. En dicho caso, lo comprobamos mediante expresiones regulares.\n",
    "\n",
    "En esta ocasión, para acotar más el número de playlists y controlar mejor aquellos caracteres con los que vamos a trabajar,  vamos a limitar el alfabeto a los siguientes caracteres:\n",
    "\n",
    "```\n",
    "' .·,:@#$€%&~°|)(><}{][+-*/%?¿!¡_–—0123456789abcdefghijklmnopqrstuvwxyzçñáéíóúàèìòùäëïöüâêîôû\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "__Importante__: Hemos ignorado los caracteres `\\` y `;` por los siguientes motivos: \n",
    "- La barra invertida, `\\`, la ignoramos para evitar las secuencias de escape (tabulador, salto de línea, etc...).\n",
    "- El punto y coma, `;`, lo ignoramos ya que lo vamos a usar para separar los campos en los archivos `.csv`\n",
    "</div> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifica aquellos títulos que no contienen letras fuera\n",
    "# del diccionario que hemos establecido.\n",
    "def contains_invalid_chars(text):\n",
    "    text = str(text).lower()\n",
    "    return any((c not in \"' .·,:@#$€%&~°|)(><}{][+-*/%?¿!¡_–—0123456789abcdefghijklmnopqrstuvwxyzçñáéíóúàèìòùäëïöüâêîôû\" and not is_emoji(c)) for c in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño df_playlists_info (con filtrado de caracteres no válidos): 2654225\n"
     ]
    }
   ],
   "source": [
    "df_playlists_info.drop(df_playlists_info[df_playlists_info['name'].apply(contains_invalid_chars)].index, inplace=True)\n",
    "print(f\"Tamaño df_playlists_info (con filtrado de caracteres no válidos): {len(df_playlists_info)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<a id=\"section445\"></a>\n",
    "#### <font color=\"#B20033\">Idioma</font> <br>\n",
    "\n",
    "Otro de los criterios que se aplicaron al crear el _Million Playlist Dataset_,  fue que las listas de reproducción pertenecían a usuarios de los Estados Unidos de América. Como la WebAPI de _Spotify_ no nos proporciona esta información, vamos a identificar el idioma de los títulos de las playlists y nos quedaremos con aquellos que estén en inglés o que contengan únicamente emoticonos y/o otros caracteres como números, signos de puntuación, etc...\n",
    "\n",
    "Para clasificar los títulos de las playlists por su idioma, se ha empleado el servicio [_Text Analytics_](https://azure.microsoft.com/services/cognitive-services/text-analytics/) de [**Azure Cognitive Services**](https://azure.microsoft.com/es-es/services/cognitive-services/).\n",
    "\n",
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> __Nota__:\n",
    "Para ver cómo funciona la detección del idioma mediante *Text Analytics*, se puede consultar un ejemplo publicado en [_Microsoft Docs_](https://docs.microsoft.com): \n",
    "- [Example: How to detect language with Text Analytics](https://docs.microsoft.com/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-language-detection)\n",
    "</div> <br>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "__Importante__: Para hacer uso del servicio, se necesita una clave de acceso. Dicha clave puede obtenerse siguiendo los pasos que se indican en el siguiente enlace:\n",
    "- [Get an access key for the Text Analytics API](https://docs.microsoft.com/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-access-key)\n",
    "</div> <br>\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> __Advertencia__:\n",
    "Este servicio de _Microsoft Azure_ es de **pago**<sup>1</sup>, pero se puede hacer uso de él mediante la instancia _'Gratis - Web/Container'_ que permite realizar 5.000 transacciones al mes de forma gratuita.\n",
    "\n",
    "<font size=1><sup>1</sup> Se recomienda consultar la [información de precios del servicio](https://azure.microsoft.com/es-es/pricing/details/cognitive-services/text-analytics/).<font>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante el método ***get_names_language***, obtenemos el idioma de cada uno de los títulos de las playlist que tenemos en nuestro DataFrame. Posteriormente los guardaremos en un fichero `.json` comprimido. Guardamos una copia de dicha información por los siguientes motivos:\n",
    "- El proceso puede durar más de 1 hora.\n",
    "- Si volvemos a realizar las llamadas al servicio, se nos vuelve a facturar por el uso que hacemos (en caso de usar alguna de las instancias de pago). En caso de hacer uso de la instancia gratuita, podríamos agotar innecesariamente las 5.000 instancias al mes que nos facilitan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar la llamada al servicio, necesitamos convertir nuestros datos al siguiente formato *JSON*: \n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "{ \n",
    "    \"documents\" : \n",
    "    [ \n",
    "        {\"id\" : \"id_0\", \"text\" : \"title_0\"},\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        {\"id\" : \"id_n\", \"text\" : \"title_n\"}\n",
    "    ] \n",
    "}\n",
    "```\n",
    "<br>\n",
    "\n",
    "Como podemos apreciar en el esquema anterior, enviamos un diccionario de un único elemento con clave '*documents*' cuyo valor es una lista de diccionarios, los textos de los cuales deseamos identificar su idioma. En una llamada al servicio podemos adjuntar hasta 1.000 textos, almacenados cada uno en un diccionario de dos elementos, dentro del fichero *JSON*. Estos elementos son:\n",
    "- ___id___ : Identificador que damos al texto.\n",
    "- ___text___ : Texto del que deseamos obtener el idioma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, mostramos el ejemplo de un JSON empleado para la llamada al servicio:\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "{\n",
    "     \"documents\": [\n",
    "         {\n",
    "             \"id\": \"1\",\n",
    "             \"text\": \"This document is in English.\"\n",
    "         },\n",
    "         {\n",
    "             \"id\": \"2\",\n",
    "             \"text\": \"Este documento está en inglés.\"\n",
    "         },\n",
    "         {\n",
    "             \"id\": \"3\",\n",
    "             \"text\": \"Ce document est en anglais.\"\n",
    "         },\n",
    "         {\n",
    "             \"id\": \"4\",\n",
    "             \"text\": \"本文件为英文\"\n",
    "         },                \n",
    "         {\n",
    "             \"id\": \"5\",\n",
    "             \"text\": \"Этот документ на английском языке.\"\n",
    "         }\n",
    "     ]\n",
    " }\n",
    "```\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names_language(df_info):\n",
    "    # Lista de diccionarios que emplearemos para almacenar\n",
    "    # el identificador y el nombre de las playlists\n",
    "    items = []\n",
    "    \n",
    "    # Forma más rápida de obtener el id y el título de cada\n",
    "    # playlist en vez de iterar sobre el DataFrame\n",
    "    index_name_tuplist = list(zip(df_info.index.to_list(), \n",
    "                                  df_info.name.to_list()))\n",
    "    \n",
    "    # Para cada elemento de la tupla, lo convertimos a\n",
    "    # un diccionario y lo añadimos a la lista 'items'\n",
    "    for tup in index_name_tuplist:\n",
    "        items.append({'id' : tup[0], 'text': tup[1]})\n",
    "\n",
    "    # Creamos bloques (chunks) de 1.000 elementos, ya que\n",
    "    # podemos hacer peticiones al servicio que contengan\n",
    "    # hasta 1.000 textos (1 texto = 1 transacción)\n",
    "    chunk_len = 1000\n",
    "    chunks_data = [items[x:x+chunk_len] for x in range(0, len(items), chunk_len)]\n",
    "    \n",
    "    # Lista que emplearemos para guardar las respuestas que\n",
    "    # nos devuelve el servicio\n",
    "    chunks_response = []\n",
    "    \n",
    "    access_key = '' # API Key de Azure Cognitive Services\n",
    "    end_point = '' # Endpoint Azure Cognitive Services\n",
    "    request_url = urllib.parse.urljoin(end_point, 'text/analytics/v2.1/languages')\n",
    "    \n",
    "    # Cabecera requerida para hacer la petición al servicio\n",
    "    headers = {'Ocp-Apim-Subscription-Key' : access_key,\n",
    "               'Content-Type' : 'application/json',\n",
    "               'Accept' : 'application/json'}\n",
    "\n",
    "    # Para cada bloque de 1.000 items, realizamos una \n",
    "    # llamada al servicio y almacenamos su respuesta\n",
    "    with tqdm(total=len(chunks_data)) as pbar:\n",
    "        for i , chunk in enumerate(chunks_data):\n",
    "            json_data = {'documents': chunk}\n",
    "            \n",
    "            res = requests.post(request_url, headers=headers, json=json_data)\n",
    "            \n",
    "            # Comprobamos que la ejecución es correcta. En caso\n",
    "            # contrario, imprimimos un mensaje con el bloque\n",
    "            # donde se ha producido el error\n",
    "            if res.status_code == 200:\n",
    "                chunks_response.append(res.json()['documents'])\n",
    "            else:\n",
    "                print(f'Error al obtener chunk[{i}]')\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Almacenamos en una única lista el resultado de la\n",
    "    # identificación del idioma de cada título que se \n",
    "    # encuentra repartido en los distintos bloques\n",
    "    language_data = []\n",
    "    \n",
    "    for chunk in chunks_response:\n",
    "        for item in chunk:\n",
    "            language_data.append(item)\n",
    "    \n",
    "    # Volcamos la información a un archivo .json\n",
    "    lang_file_name = 'lang_results_list.json'\n",
    "    lang_file_path = os.path.join(BACKUP_PATH, lang_file_name)\n",
    "    with open(lang_file_path, 'w') as file:\n",
    "        json.dump(language_data,file,indent=4)\n",
    "        \n",
    "    # Comprimimos el fichero .json y eliminamos el fichero .json\n",
    "    lang_zip_path = os.path.join(BACKUP_PATH,'lang_results_list.zip')\n",
    "    with zipfile.ZipFile(lang_zip_path,'w') as zip_file: \n",
    "         zip_file.write(lang_file_path, compress_type=zipfile.ZIP_DEFLATED, arcname=lang_file_name)\n",
    "        \n",
    "    os.remove(lang_file_path)\n",
    "    \n",
    "    return language_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "El formato de la respuesta que obtendremos tras la llamada al servicio _language_ de _Text Analytics_ es la siguiente:\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "```\n",
    "{ \n",
    "    \"documents\" : \n",
    "    [ \n",
    "        {\"id\" : \"id_0\",\n",
    "         \"detectedLanguages\": \n",
    "         [\n",
    "             {\n",
    "                 \"name\": \"language_0\",\n",
    "                 \"iso6391Name\": \"l_0\",\n",
    "                 \"score\": s_0\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        {\"id\" : \"id_n\",\n",
    "         \"detectedLanguages\": \n",
    "         [\n",
    "             {\n",
    "                 \"name\": \"language_n\",\n",
    "                 \"iso6391Name\": \"l_n\",\n",
    "                 \"score\": s_n\n",
    "            }\n",
    "        ]\n",
    "    ] \n",
    "}\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "La respuesta es similar a la llamada, salvo que en este caso el segundo valor del diccionario es otra lista de diccionarios, llamada '*detectedLanguages*' , con el idioma que ha identificado (en dos formatos) y su puntuación. Los elementos que componen la identificación al idioma del texto, con el _id_ que hemos establecido, son los siguientes:\n",
    "- ***name*** : Nombre del idioma identificado.\n",
    "- ***iso6391Name*** : Nombre del idioma identificado en formato [_ISO 639-1_](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes).\n",
    "- ***score*** : Puntuación obtenida por la identificación.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, mostramos la respuesta que se obtendría tras la identificación de los textos del ejemplo anterior:\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "{\n",
    "    \"documents\": [\n",
    "        {\n",
    "            \"id\": \"1\",\n",
    "            \"detectedLanguages\": [\n",
    "                {\n",
    "                    \"name\": \"English\",\n",
    "                    \"iso6391Name\": \"en\",\n",
    "                    \"score\": 1\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"2\",\n",
    "            \"detectedLanguages\": [\n",
    "                {\n",
    "                    \"name\": \"Spanish\",\n",
    "                    \"iso6391Name\": \"es\",\n",
    "                    \"score\": 1\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"3\",\n",
    "            \"detectedLanguages\": [\n",
    "                {\n",
    "                    \"name\": \"French\",\n",
    "                    \"iso6391Name\": \"fr\",\n",
    "                    \"score\": 1\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"4\",\n",
    "            \"detectedLanguages\": [\n",
    "                {\n",
    "                    \"name\": \"Chinese_Simplified\",\n",
    "                    \"iso6391Name\": \"zh_chs\",\n",
    "                    \"score\": 1\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"5\",\n",
    "            \"detectedLanguages\": [\n",
    "                {\n",
    "                    \"name\": \"Russian\",\n",
    "                    \"iso6391Name\": \"ru\",\n",
    "                    \"score\": 1\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Realizamos el proceso de obtención del idioma de los títulos. En caso de existir un fichero comprimido con el nombre `lang_results_list.zip` lo cargamos, puesto que contiene la información que necesitamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha obtenido la información del idioma para 2659470 títulos\n"
     ]
    }
   ],
   "source": [
    "lang_zip_path = os.path.join(BACKUP_PATH,'lang_results_list.zip')\n",
    "\n",
    "if os.path.isfile(lang_zip_path):\n",
    "    with zipfile.ZipFile(lang_zip_path,'r') as zip_file:\n",
    "        with zip_file.open(zip_file.namelist()[0]) as zip_file:\n",
    "            data = zip_file.read()\n",
    "            language_data = json.loads(data.decode())\n",
    "else:\n",
    "    language_data = get_names_language(df_playlists_info[0:10])\n",
    "    \n",
    "print(f\"Se ha obtenido la información del idioma para {len(language_data)} títulos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "A continuación, vamos a crear un DataFrame llamado ***df_language*** con el que realizaremos el filtrado de playlists mediante el idioma del título. Este DataFrame contendrá las siguientes columnas:\n",
    "\n",
    "- ***id***: Identificador de la playlist.\n",
    "- ***name***: Título de la playlist.\n",
    "- ***iso6391Name***: Nombre del idioma identificado en formato [_ISO 639-1_](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes).\n",
    "- ***score***: Puntuación obtenida por la identificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dict = dict()\n",
    "\n",
    "for item in language_data:\n",
    "    language_dict[item['id']] = item['detectedLanguages'][0]\n",
    "\n",
    "df_language = pd.DataFrame.from_dict(language_dict, orient='index')\n",
    "\n",
    "df_language.drop(columns=['name'],inplace=True)\n",
    "\n",
    "df_language = pd.concat([df_playlists_info['name'], df_language],axis=1, sort=False)\n",
    "df_language.index.name = 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>iso6391Name</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3FI0aZZz9hUw5qK0K2GDwH</th>\n",
       "      <td>Love Letter</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381M0rlWt8mB0CayapCcRr</th>\n",
       "      <td>skin deep</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3nSWAjR9QuXhb3YqO24HHw</th>\n",
       "      <td>junior year</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5M6FDo7oQ33CvNMaMiZ1uH</th>\n",
       "      <td>BOOM BOOM</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43QPMRPsHlbXILXIhYoxF6</th>\n",
       "      <td>Born in the Wrong Era</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name iso6391Name  score\n",
       "id                                                              \n",
       "3FI0aZZz9hUw5qK0K2GDwH            Love Letter          en    1.0\n",
       "381M0rlWt8mB0CayapCcRr              skin deep          en    1.0\n",
       "3nSWAjR9QuXhb3YqO24HHw           junior year           en    1.0\n",
       "5M6FDo7oQ33CvNMaMiZ1uH              BOOM BOOM          en    1.0\n",
       "43QPMRPsHlbXILXIhYoxF6  Born in the Wrong Era          en    1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_language.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Como tenemos títulos que sólo contienen emoticonos, los cuales han sido identificados como '_(Unknown)_', vamos a identificarlos con la etiqueta '_(Emoji)_' y a darles una puntuación de _1_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_language.loc[(df_language['iso6391Name'] == '(Unknown)') & \n",
    "                (df_language.name.apply(all_text_emoji)), ['iso6391Name']] = '(Emoji)'\n",
    "df_language.loc[df_language['iso6391Name'] == '(Emoji)',['score']] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Nuestro principal objetivo es quedaros con aquellos títulos que estén en _inglés_, pero también vamos a seleccionar aquellos títulos cuya etiqueta de idioma sea _('Emoji')_ y _('Unknown')_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_language.drop(df_language[(df_language['iso6391Name'] != 'en') &\n",
    "                             (df_language['iso6391Name'] != '(Emoji)') &\n",
    "                             (df_language['iso6391Name'] != '(Unknown)')].index,\n",
    "                 inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "También vamos a eliminar aquellos títulos para los cuales su _score es inferior a 0.75_. Esta medida sólo la aplicaremos para los títulos en los que el idioma haya sido identificado como '_en_' (inglés):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_language.drop(df_language[(df_language['iso6391Name'] == 'en') &\n",
    "                             (df_language['score'] < 0.75)].index,\n",
    "                 inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Una vez hemos seleccionado aquellas playlist que nos son de interés, obtenemos sus _id_ y en el DataFrame `df_playlists_info` descartamos aquellas que no se encuentren en el DataFrame `df_language`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño df_playlists_info (con filtrado de idioma): 2485960\n"
     ]
    }
   ],
   "source": [
    "df_playlists_info = df_playlists_info[df_playlists_info.index.isin(df_language.index)]\n",
    "print(f'Tamaño df_playlists_info (con filtrado de idioma): {len(df_playlists_info)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section446\"></a>\n",
    "#### <font color=\"#B20033\">Títulos ofensivos</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "En este último filtrado, vamos a eliminar los títulos que resulten _ofensivos_. Consideraremos que los títulos ofensivos son aquellos que contienen una o varias palabras como insultos, palabras que pertenezcan a temas considerados _sensibles_, etc...\n",
    "\n",
    "Tras obtener un diccionario de palabras ofensivas de [_Free Web Headers_](https://www.freewebheaders.com/full-list-of-bad-words-banned-by-google/), vamos a eliminar aquellas playlists que contengan alguna de estas palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = []\n",
    "\n",
    "with open('files/bad-words.txt') as f:\n",
    "    bad_words = f.read().splitlines()\n",
    "\n",
    "# Palabra ofensiva (única)\n",
    "single_bad_words = [word for word in bad_words if len(word.split()) == 1]\n",
    "# Palabras ofensivas (2 o más)\n",
    "compound_bad_words = [word for word in bad_words if len(word.split()) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_single_bad_name(name):\n",
    "    return len([word for word in name.split(' ') if word in single_bad_words]) > 0\n",
    "\n",
    "def is_compound_bad_name(name):\n",
    "    is_bad = False\n",
    "    for word in compound_bad_words:\n",
    "        if word in name:\n",
    "            is_bad = True\n",
    "            break\n",
    "    return is_bad\n",
    "\n",
    "def is_bad_name(name):\n",
    "    name = str(name).lower() \n",
    "    return is_single_bad_name(name) or is_compound_bad_name(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 2485960/2485960 [02:19<00:00, 17819.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño df_playlists (con filtrado de títulos ofensivos): 2392399\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "df_playlists_info.drop(df_playlists_info[df_playlists_info['name'].progress_apply(is_bad_name)].index, inplace=True)\n",
    "print(f'Tamaño df_playlists (con filtrado de títulos ofensivos): {len(df_playlists_info)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> __Nota__:\n",
    "Como el proceso de detección de títulos ofensivos puede tardar varios minutos, hemos añadido una barra de progreso. La documentación de la barra de progreso de _tqdm_ para _pandas_ podemos encontrarla en el siguiente enlace:\n",
    "- [tqdm: Pandas Integration](https://pypi.org/project/tqdm/#pandas-integration).\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section45\"></a>\n",
    "### <font color=\"#B20033\">4.5 - Consideraciones en el filtrado de playlists</font>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras realizar el filtrado de playlists, con los criterios que habíamos establecido, no hemos conseguido reducir lo suficiente el número de listas, vamos a aplicar los nuevos criterios que se hemos indicado previamente.\n",
    "\n",
    "<br>\n",
    "\n",
    "Como la variable *num_followers* de nuestro conjunto podría ser considerada como una de las más importantes, aumentamos el mínimo de seguidores que tiene una playlist de 1 a 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño df_playlists_info (num_followers > 1): 1274514\n"
     ]
    }
   ],
   "source": [
    "df_playlists_info.drop(df_playlists_info[df_playlists_info['num_followers'] < 2].index,inplace=True)\n",
    "print(f'Tamaño df_playlists_info (num_followers > 1): {len(df_playlists_info)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puesto que todavía no hemos conseguido que el número de playlists esté próximo a 1.000.000, vamos a eliminar aquellas listas donde aparezcan artistas poco frecuentes. Es decir, vamos a comprobar el número de veces que aparece un determinado artista en el conjunto de playlists y si es igual a 1, eliminamos la playlists que lo contiene.\n",
    "\n",
    "Este proceso se hará de forma recursiva, ya que cuando borremos aquellas playlists con artistas que aparecen una única vez, puede que volvamos a tener listas con éste mismo criterio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el archivo que contiene la información de los artistas que contiene una playlists\n",
    "df_pls_artists = pd.read_csv(os.path.join(DATA_PATH,'downloaded_pls_artists.csv'), sep=\";\", header=None)\n",
    "df_pls_artists.columns = ['id', 'artists']\n",
    "df_pls_artists.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos aquellas playlist que se han descartado durante el proceso de filtrado\n",
    "df_pls_artists = df_pls_artists.loc[df_playlists_info.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Creamos un diccionario en el cual almacenaremos como clave el identificador del artista y cuyo valor será una lista de playlist donde aparece dicho artista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4dc5e39eccc42a0b9655586e471d067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1274514), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "artists_pls_dict = defaultdict(list)\n",
    "\n",
    "pbar = tqdm_nb(total=len(df_pls_artists))\n",
    "for r_id , r_data in df_pls_artists.iterrows():\n",
    "    pl_id = r_id\n",
    "    artists = r_data['artists'].split('|')\n",
    "    for artist in artists:\n",
    "        artists_pls_dict[artist].append(pl_id)\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de artistas en el conjunto de datos: 763499\n"
     ]
    }
   ],
   "source": [
    "print(f'Número de artistas en el conjunto de datos: {len(artists_pls_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevo número de artistas en el conjunto de datos: 237560\n"
     ]
    }
   ],
   "source": [
    "# Proceso de filtrado de playlists con artistas poco frecuentes\n",
    "end = False\n",
    "\n",
    "while not end:\n",
    "    artists_set = set()\n",
    "    removable_artists = []\n",
    "    \n",
    "    for k,v in artists_pls_dict.items():\n",
    "        if len(v) <= 1:\n",
    "            artists_set.update(v)\n",
    "            removable_artists.append(k)     \n",
    "            \n",
    "    if len(removable_artists) == 0 and len(artists_set) == 0:\n",
    "        end = True\n",
    "    else:\n",
    "        for artist in removable_artists:\n",
    "            del artists_pls_dict[artist]\n",
    "        for k,v in artists_pls_dict.items():\n",
    "            artists_pls_dict[k] = [x for x in v if x not in artists_set]\n",
    "            \n",
    "print(f'Nuevo número de artistas en el conjunto de datos: {len(artists_pls_dict)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, extraemos los identificadores de las playlists del diccionario y filtramos *df_playlists_info* con las listas que hemos descartado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño df_playlists_info (con filtrado de artistas): 1019338\n"
     ]
    }
   ],
   "source": [
    "playlists_set = set()\n",
    "\n",
    "for _ ,v in artists_pls_dict.items():\n",
    "    playlists_set.update(v)\n",
    "    \n",
    "df_playlists_info = df_playlists_info.loc[list(playlists_set)]\n",
    "print(f'Tamaño df_playlists_info (con filtrado de artistas): {len(df_playlists_info)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras aplicar estos últimos filtros, nos hemos quedado con un número de playlists muy próximo a 1.000.000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section5\"></a>\n",
    "## <font color=\"#92002A\">5 - Almacenamiento de resultados</font>\n",
    "<br>\n",
    "\n",
    "Lo primero que vamos a hacer es aleatorizar las filas del DataFrame `df_playlists_info`, estableciendo la semilla a _1_.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> __Nota__:\n",
    "En el proceso de generación del conjunto de datos, utilizaremos las playlists adicionales que hemos obtenido para crear el conjunto de prueba.\n",
    "</div> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_playlists_info = df_playlists_info.sample(n=len(df_playlists_info), random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Si mostramos las primeras 5 columnas que contiene el DataFrame, vemos que las columnas *num_followers*, *num_edits* y *modified_at* son de tipo `float`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collaborative</th>\n",
       "      <th>name</th>\n",
       "      <th>num_tracks</th>\n",
       "      <th>num_followers</th>\n",
       "      <th>num_artists</th>\n",
       "      <th>num_albums</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>modified_at</th>\n",
       "      <th>num_edits</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7u1RhzyK1ykPQ1wfemcqoR</th>\n",
       "      <td>False</td>\n",
       "      <td>Low viscosity vibes</td>\n",
       "      <td>58</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25</td>\n",
       "      <td>38</td>\n",
       "      <td>15191903</td>\n",
       "      <td>1.559722e+09</td>\n",
       "      <td>14.0</td>\n",
       "      <td>pls-set_07672.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0ZcZqlDEw4eLILfHdni8vG</th>\n",
       "      <td>False</td>\n",
       "      <td>dalanda 🐉</td>\n",
       "      <td>104</td>\n",
       "      <td>4.0</td>\n",
       "      <td>92</td>\n",
       "      <td>100</td>\n",
       "      <td>22361466</td>\n",
       "      <td>1.558851e+09</td>\n",
       "      <td>75.0</td>\n",
       "      <td>pls-set_05170.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0Z48DgMwRPZVVTF7Y8RLB6</th>\n",
       "      <td>False</td>\n",
       "      <td>freeze pops</td>\n",
       "      <td>52</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>10643802</td>\n",
       "      <td>1.552264e+09</td>\n",
       "      <td>7.0</td>\n",
       "      <td>pls-set_06675.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1G8V8nWYp6uVacS8XAPSbo</th>\n",
       "      <td>False</td>\n",
       "      <td>Golden Oldies</td>\n",
       "      <td>98</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64</td>\n",
       "      <td>90</td>\n",
       "      <td>25418594</td>\n",
       "      <td>1.556949e+09</td>\n",
       "      <td>34.0</td>\n",
       "      <td>pls-set_01145.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1mVzj3AosDkLdJOQEtfwaA</th>\n",
       "      <td>False</td>\n",
       "      <td>I d◻n't g◻v◻  a f◻ck</td>\n",
       "      <td>224</td>\n",
       "      <td>2.0</td>\n",
       "      <td>89</td>\n",
       "      <td>154</td>\n",
       "      <td>46528637</td>\n",
       "      <td>1.558704e+09</td>\n",
       "      <td>82.0</td>\n",
       "      <td>pls-set_06752.zip</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        collaborative                  name  num_tracks  \\\n",
       "id                                                                        \n",
       "7u1RhzyK1ykPQ1wfemcqoR          False   Low viscosity vibes          58   \n",
       "0ZcZqlDEw4eLILfHdni8vG          False             dalanda 🐉         104   \n",
       "0Z48DgMwRPZVVTF7Y8RLB6          False           freeze pops          52   \n",
       "1G8V8nWYp6uVacS8XAPSbo          False         Golden Oldies          98   \n",
       "1mVzj3AosDkLdJOQEtfwaA          False  I d◻n't g◻v◻  a f◻ck         224   \n",
       "\n",
       "                        num_followers  num_artists  num_albums  duration_ms  \\\n",
       "id                                                                            \n",
       "7u1RhzyK1ykPQ1wfemcqoR            3.0           25          38     15191903   \n",
       "0ZcZqlDEw4eLILfHdni8vG            4.0           92         100     22361466   \n",
       "0Z48DgMwRPZVVTF7Y8RLB6            2.0           13          25     10643802   \n",
       "1G8V8nWYp6uVacS8XAPSbo            2.0           64          90     25418594   \n",
       "1mVzj3AosDkLdJOQEtfwaA            2.0           89         154     46528637   \n",
       "\n",
       "                         modified_at  num_edits          file_name  \n",
       "id                                                                  \n",
       "7u1RhzyK1ykPQ1wfemcqoR  1.559722e+09       14.0  pls-set_07672.zip  \n",
       "0ZcZqlDEw4eLILfHdni8vG  1.558851e+09       75.0  pls-set_05170.zip  \n",
       "0Z48DgMwRPZVVTF7Y8RLB6  1.552264e+09        7.0  pls-set_06675.zip  \n",
       "1G8V8nWYp6uVacS8XAPSbo  1.556949e+09       34.0  pls-set_01145.zip  \n",
       "1mVzj3AosDkLdJOQEtfwaA  1.558704e+09       82.0  pls-set_06752.zip  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_playlists_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como los valores de dichas variables siempre son números enteros, vamos a cambiarlas su tipo a `int64`. También vamos a establecer la variable *collaborative* a tipo `bool`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1019338 entries, 7u1RhzyK1ykPQ1wfemcqoR to 03aYLTK68a6Dy6twBW94tY\n",
      "Data columns (total 10 columns):\n",
      "collaborative    1019338 non-null bool\n",
      "name             1019338 non-null object\n",
      "num_tracks       1019338 non-null int64\n",
      "num_followers    1019338 non-null int64\n",
      "num_artists      1019338 non-null int64\n",
      "num_albums       1019338 non-null int64\n",
      "duration_ms      1019338 non-null int64\n",
      "modified_at      1019338 non-null int64\n",
      "num_edits        1019338 non-null int64\n",
      "file_name        1019338 non-null object\n",
      "dtypes: bool(1), int64(7), object(2)\n",
      "memory usage: 78.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_playlists_info.num_followers = df_playlists_info.num_followers.astype('int64')\n",
    "df_playlists_info.num_edits = df_playlists_info.num_edits.astype('int64')\n",
    "df_playlists_info.modified_at = df_playlists_info.modified_at.astype('int64')\n",
    "df_playlists_info.collaborative = df_playlists_info.collaborative.astype('bool')\n",
    "\n",
    "df_playlists_info.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras aleatorizar las filas del *DataFrame* y corregir el tipo de datos de las columnas, procedemos a almacenar el conjunto con la información de las playlists que vamos a emplear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'backup\\\\mpd_info_set.csv'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.path.join(DATA_PATH,'mpd_info_set.csv')       \n",
    "df_playlists_info.to_csv(file_path, sep=';', encoding='utf-8')\n",
    "\n",
    "# Guardamos una copia de seguridad\n",
    "copyfile(file_path, os.path.join(BACKUP_PATH, 'mpd_info_set.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-graduation-cap\" aria-hidden=\"true\" style=\"color:#92002A\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
