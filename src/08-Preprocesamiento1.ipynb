{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/header.png\" alt=\"Logo UCLM-ESII\" align=\"right\">\n",
    "\n",
    "<br><br><br><br>\n",
    "<h2><font color=\"#92002A\" size=4>Trabajo Fin de Grado</font></h2>\n",
    "\n",
    "<h1><font color=\"#6B001F\" size=5>Generación automática de playlist de canciones <br> mediante técnicas de minería de datos</font></h1>\n",
    "<h2><font color=\"#92002A\" size=3>Parte 8 - Preprocesamiento 1</font></h2>\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: right\">\n",
    "    <font color=\"#B20033\" size=3><strong>Autor</strong>: <em>Miguel Ángel Cantero Víllora</em></font><br>\n",
    "    <br>\n",
    "    <font color=\"#B20033\" size=3><strong>Directores</strong>: <em>José Antonio Gámez Martín</em></font><br>\n",
    "    <font color=\"#B20033\" size=3><em>Juan Ángel Aledo Sánchez</em></font><br>\n",
    "    <br>\n",
    "<font color=\"#B20033\" size=3>Grado en Ingeniería Informática</font><br>\n",
    "<font color=\"#B20033\" size=2>Escuela Superior de Ingeniería Informática | Universidad de Castilla-La Mancha</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#92002A\" size=5>Índice</font></h2>\n",
    "\n",
    "<br>\n",
    "\n",
    "* [1. Introducción](#section1)\n",
    "* [2. Cambio de formato del DataSet](#section2)\n",
    "* [3. Modificaciones adicionales en DataSet](#section3)\n",
    "* [4. Creación de características de playlist](#section4)\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Permite establecer la anchura de la celda\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import emoji\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm.notebook import tqdm as tqdm_nb\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar si es la primera vez que se emplea nltk\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('punkt') \n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MPD_PATH = 'MPD'\n",
    "MPD_TEST_PATH = 'MPD_TEST'\n",
    "MPD_CSV_PATH = 'MPD_CSV'\n",
    "MPD_SLICE_PREFIX = 'mpd.slice.'\n",
    "\n",
    "ALBUMS_FILE = os.path.join(MPD_CSV_PATH,'mpd.albums.csv')\n",
    "ARTISTS_FILE = os.path.join(MPD_CSV_PATH,'mpd.artists.csv')\n",
    "TRACKS_FILE = os.path.join(MPD_CSV_PATH,'mpd.tracks.csv')\n",
    "PLSTRS_FILE = os.path.join(MPD_CSV_PATH,'mpd.pls-tracks.csv')\n",
    "PLSTARTS_FILE = os.path.join(MPD_CSV_PATH,'mpd.pls-artists.csv')\n",
    "PLSINFO_FILE = os.path.join(MPD_CSV_PATH,'mpd.playlists-info.csv')\n",
    "PLSTESTINFO_FILE = os.path.join(MPD_CSV_PATH,'mpd.playlists-info-test.csv')\n",
    "NAMES_FILE = os.path.join(MPD_CSV_PATH, \"mpd.names.csv\")\n",
    "PLSTRS_PREFIX = 'mpd.playlists-tracks.'\n",
    "EMOJI_DICT_FILE = os.path.join(MPD_CSV_PATH, \"emojis_translation_dict.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<a id=\"section1\"></a>\n",
    "## <font color=\"#92002A\">1 - Introducción</font>\n",
    "<br>\n",
    "\n",
    "En esta libreta vamos a convertir el *dataset* que hemos obtenido previamente, en formato *JSON*, a formato *CSV*. También vamos a generar las características de las playlists para usar en el modelo de *LightFM* que crearemos más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "## <font color=\"#92002A\">2 - Cambio de formato del DataSet</font>\n",
    "<br>\n",
    "\n",
    "Cuando creamos los ficheros que contienen el dataset con el conjunto de playlists, empleamos el formato JSON y lo almacenamos en varios archivos comprimidos debido a su elevado tamaño: 7,5 GB (43,3 GB sin comprimir). También nos encontramos con el caso de que tenemos la información repetida de las pistas, puesto que cada playlist contiene toda la información completa de éstas.\n",
    "\n",
    "\n",
    "Para solucionar estos problemas y poder trabajar mejor con los datos, vamos a convertirlos al formato CSV, con lo que tendremos la información repartida en varias tablas y evitaremos tener información repetida de forma innecesaria. Se ha creado un fichero/tabla para cada tipo de item del conjunto:\n",
    "\n",
    "*\tÁlbumes.\n",
    "*\tArtistas.\n",
    "*\tPistas.\n",
    "*\tInformación sobre las playlists.\n",
    "*\tInformación sobre las playlists del conjunto de prueba.\n",
    "*\tLista de pistas de las playlists (para ambos conjuntos).\n",
    "\n",
    "Adicionalmente, hemos creado una tabla que contiene la lista de artistas de las playlists, junto al número de veces que aparece cada artista en ellas.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"section21\"></a>\n",
    "### <font color=\"#B20033\">2.1 - Definición de funciones adicionales</font>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que nos permite leer un archivo .json comprimido o sin comprimir\n",
    "# y devuelve un diccionario con su contenido\n",
    "def json_file_reader(file_path):\n",
    "    \"\"\"\n",
    "    :param file_path: Ruta del fichero a leer.\n",
    "    :return results: Diccionario con los datos leidos del fichero JSON.\n",
    "    \"\"\"\n",
    "    _ , file_extension = os.path.splitext(file_path)\n",
    "\n",
    "    # Fichero comprimido\n",
    "    if file_extension == '.zip':\n",
    "        with ZipFile(file_path,'r') as zip_file:\n",
    "            with zip_file.open(zip_file.namelist()[0]) as json_file:\n",
    "                json_data = json.load(json_file)\n",
    "    # Fichero sin comprimir\n",
    "    elif file_extension == '.json':\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            json_data = json.load(json_file)            \n",
    "    # En caso de que sea otra extensión, devolvemos un diccionario vacío\n",
    "    else:\n",
    "        json_data = {}            \n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que se encarga de convertir el dataset que contiene \n",
    "# 1 millón de playlists a formato CSV\n",
    "def jsonds_to_csvds(json_ds_path,csv_ds_path):\n",
    "    \"\"\"\n",
    "    :param json_ds_path: Ruta donde se encuentra el conjunto de datos en formato JSON.\n",
    "    :param csv_ds_path: Ruta donde almacenaremos el nuevo conjunto de datos en formato CSV.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(csv_ds_path):\n",
    "        os.mkdir(csv_ds_path)    \n",
    "    \n",
    "    files = []\n",
    "    tracks_dict = defaultdict(dict)\n",
    "\n",
    "    for file in os.listdir(json_ds_path):\n",
    "        if file.startswith(MPD_SLICE_PREFIX):\n",
    "            files.append(os.path.join(json_ds_path,file))\n",
    "\n",
    "    plstrs_fieldnames = ['pid','pos','track_uri']\n",
    "    tracks_fieldnames = ['track_name', 'track_uri', 'duration_ms', 'artist_name', \n",
    "                         'artist_uri', 'album_name', 'album_uri']\n",
    "    plsinfo_fieldnames = ['pid','name','collaborative','modified_at',\n",
    "                          'num_albums','num_tracks', 'num_followers',\n",
    "                          'num_edits','duration_ms','num_artists']\n",
    "\n",
    "    with open(PLSINFO_FILE,'w',newline='') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=plsinfo_fieldnames,delimiter=',')\n",
    "        writer.writeheader()\n",
    "\n",
    "    for file in tqdm_nb(files):\n",
    "        file_name , _ = os.path.splitext(file)\n",
    "        portion = file_name.split('.')[-1]\n",
    "        csv_pltrs_file = os.path.join(csv_ds_path, f\"{PLSTRS_PREFIX}{portion}.csv\")\n",
    "        row_list = []\n",
    "\n",
    "        with open(PLSINFO_FILE,'a',encoding='utf8',newline='') as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=plsinfo_fieldnames,\n",
    "                                    delimiter=',',quoting=csv.QUOTE_MINIMAL)\n",
    "            for pl in json_file_reader(file)['playlists']:\n",
    "                tracks_list = pl.pop('tracks')\n",
    "                writer.writerow(pl)\n",
    "                for track in tracks_list:\n",
    "                    pos = track.pop('pos')\n",
    "                    row = {'pid': pl['pid'], 'pos': pos,\n",
    "                           'track_uri' : track['track_uri']}\n",
    "                    row_list.append(row)\n",
    "                    tracks_dict[track['track_uri']] = track\n",
    "\n",
    "        with open(csv_pltrs_file,'w',newline='') as csv_tracks_file:\n",
    "            writer_tracks = csv.DictWriter(csv_tracks_file, fieldnames=plstrs_fieldnames)\n",
    "            writer_tracks.writeheader()\n",
    "            for row in row_list:\n",
    "                writer_tracks.writerow(row)\n",
    "    \n",
    "    print(\"Volcado de información sobre pistas:\")\n",
    "    with open(TRACKS_FILE,'w',newline='', encoding='utf8') as csv_tracks_file:\n",
    "        writer_tracks = csv.DictWriter(csv_tracks_file, fieldnames=tracks_fieldnames)\n",
    "        writer_tracks.writeheader()\n",
    "        pbar = tqdm_nb(total=len(tracks_dict))\n",
    "        for track in tracks_dict.values():\n",
    "            writer_tracks.writerow(track)\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que se encarga de convertir el conjunto de datos (test)\n",
    "# a formato CSV\n",
    "def jsonds_to_csvds_test(json_ds_path,csv_ds_path):\n",
    "    \"\"\"\n",
    "    :param json_ds_path: Ruta donde se encuentra el conjunto de datos en formato JSON.\n",
    "    :param csv_ds_path: Ruta donde almacenaremos el nuevo conjunto de datos en formato CSV.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(csv_ds_path):\n",
    "        os.mkdir(csv_ds_path)\n",
    "\n",
    "    file_name = \"mpd.test.zip\"\n",
    "    \n",
    "    if file_name in os.listdir(json_ds_path):\n",
    "        plstrs_fieldnames = ['pid','pos','track_uri']\n",
    "        plsinfo_fieldnames = ['pid','name','num_holdouts','num_samples',\n",
    "                              'num_tracks']\n",
    "        row_list = []\n",
    "        \n",
    "        with open(PLSTESTINFO_FILE, 'w',newline='',encoding='utf8') as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=plsinfo_fieldnames,\n",
    "                                    delimiter=',',quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            test_playlists = json_file_reader(os.path.join(json_ds_path,file_name))['playlists']\n",
    "            for pl in test_playlists:\n",
    "                tracks_list = pl.pop('tracks')\n",
    "                writer.writerow(pl)\n",
    "                \n",
    "                for track in tracks_list:\n",
    "                    pos = track.pop('pos')\n",
    "                    row = {'pid': pl['pid'], 'pos': pos,\n",
    "                           'track_uri' : track['track_uri']}\n",
    "                    row_list.append(row)\n",
    "        \n",
    "        plstrs_test_path = os.path.join(MPD_CSV_PATH,\"{}test.csv\".format(PLSTRS_PREFIX))\n",
    "        with open(plstrs_test_path, 'w', newline='', encoding='utf8') as csv_tracks_file:\n",
    "            writer_tracks = csv.DictWriter(csv_tracks_file, fieldnames=plstrs_fieldnames)\n",
    "            writer_tracks.writeheader()\n",
    "            for row in row_list:\n",
    "                writer_tracks.writerow(row)    \n",
    "    else:\n",
    "        print(\"ERROR: Fichero del conjunto de prueba no encontrado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonds_to_csvds(MPD_PATH,MPD_CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonds_to_csvds_test(MPD_TEST_PATH,MPD_CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras este proceso el conjunto queda convertido a formato *CSV*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "## <font color=\"#92002A\">3 - Modificaciones adicionales en DataSet</font>\n",
    "<br>\n",
    "\n",
    "Como los identificadores Spotify contienen unos “prefijos” que resultan únicos en el caso de los álbumes, artistas y pistas, los eliminamos (ya que en caso de ser necesario podemos volver a añadirlos). Ejemplo:\n",
    "\n",
    "_spotify:album:**0MlTOiC5ZYKFGeZ8h3D4rd**_ => *0MlTOiC5ZYKFGeZ8h3D4rd*\n",
    "\n",
    "\n",
    "Para que las tablas sean más fáciles de entender y ocupen menos tamaño, hemos establecido nuestro propio identificador, al que llamaremos PID, para álbumes, pistas y artistas. De esta manera las relaciones entre los elementos de las tablas, que hemos almacenado en ficheros CSV, se hacen empleando estos identificadores, y no por el código alfanumérico de Spotify\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_name</th>\n",
       "      <th>track_uri</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>artist_uri</th>\n",
       "      <th>album_name</th>\n",
       "      <th>album_uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Control</td>\n",
       "      <td>spotify:track:3G0MlTNw2AX3Xdbhrj33OS</td>\n",
       "      <td>237306</td>\n",
       "      <td>Holland</td>\n",
       "      <td>spotify:artist:21KpdYIquYJUEiEcPO2cnI</td>\n",
       "      <td>No Control</td>\n",
       "      <td>spotify:album:5F9BLbIkEvApFC8flNVKYR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>City Lights</td>\n",
       "      <td>spotify:track:3R9H16eUSv5vJ9DEgMG2Lu</td>\n",
       "      <td>369693</td>\n",
       "      <td>Holland</td>\n",
       "      <td>spotify:artist:21KpdYIquYJUEiEcPO2cnI</td>\n",
       "      <td>No Control</td>\n",
       "      <td>spotify:album:5F9BLbIkEvApFC8flNVKYR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sunshine</td>\n",
       "      <td>spotify:track:1YgaIKDgm7IQpq9MVAmExQ</td>\n",
       "      <td>252066</td>\n",
       "      <td>Keane</td>\n",
       "      <td>spotify:artist:53A0W3U0s8diEn9RhXQhVz</td>\n",
       "      <td>Hopes And Fears</td>\n",
       "      <td>spotify:album:0MlTOiC5ZYKFGeZ8h3D4rd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    track_name                             track_uri  duration_ms artist_name  \\\n",
       "0   No Control  spotify:track:3G0MlTNw2AX3Xdbhrj33OS       237306     Holland   \n",
       "1  City Lights  spotify:track:3R9H16eUSv5vJ9DEgMG2Lu       369693     Holland   \n",
       "2     Sunshine  spotify:track:1YgaIKDgm7IQpq9MVAmExQ       252066       Keane   \n",
       "\n",
       "                              artist_uri       album_name  \\\n",
       "0  spotify:artist:21KpdYIquYJUEiEcPO2cnI       No Control   \n",
       "1  spotify:artist:21KpdYIquYJUEiEcPO2cnI       No Control   \n",
       "2  spotify:artist:53A0W3U0s8diEn9RhXQhVz  Hopes And Fears   \n",
       "\n",
       "                              album_uri  \n",
       "0  spotify:album:5F9BLbIkEvApFC8flNVKYR  \n",
       "1  spotify:album:5F9BLbIkEvApFC8flNVKYR  \n",
       "2  spotify:album:0MlTOiC5ZYKFGeZ8h3D4rd  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tracks = pd.read_csv(TRACKS_FILE)\n",
    "df_tracks.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artists = df_tracks[['artist_name', 'artist_uri']].copy()\n",
    "df_artists.drop_duplicates(subset=['artist_uri'],inplace=True)\n",
    "df_artists.reset_index(drop=True, inplace=True)\n",
    "df_artists.index.name = 'artist_pid'\n",
    "df_artists['artist_id'] = df_artists['artist_uri'].apply(lambda x: x.split(':')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_albums = df_tracks[['album_name', 'album_uri', 'artist_uri']].copy()\n",
    "df_albums.drop_duplicates(subset=['album_uri'],inplace=True)\n",
    "df_albums.reset_index(drop=True, inplace=True)\n",
    "df_albums.index.name = 'album_pid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tracks.drop(columns=['artist_name','album_name'],inplace=True)\n",
    "df_tracks.drop_duplicates(subset='track_uri',inplace=True)\n",
    "df_tracks.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tracks = pd.merge(df_tracks, df_artists.reset_index()[['artist_pid','artist_uri']], on=['artist_uri']) \\\n",
    "                .drop(columns=['artist_uri'])\n",
    "df_tracks = pd.merge(df_tracks, df_albums.reset_index()[['album_pid','album_uri']], on=['album_uri']) \\\n",
    "                .drop(columns=['album_uri'])\n",
    "\n",
    "df_tracks.index.name = 'track_pid'\n",
    "\n",
    "df_tracks['track_id'] = df_tracks['track_uri'].apply(lambda x: x.split(':')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_albums = pd.merge(df_albums, df_artists.reset_index()[['artist_pid','artist_uri']], on=['artist_uri']) \\\n",
    "                .drop(columns=['artist_uri'])\n",
    "df_albums.index.name = 'album_pid'\n",
    "\n",
    "df_albums['album_id'] = df_albums['album_uri'].apply(lambda x: x.split(':')[-1])\n",
    "df_albums.drop(columns=['album_uri'], inplace = True)\n",
    "df_artists.drop(columns=['artist_uri'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función empleada para leer un dataframe que ha sido almacenado\n",
    "# en varios ficheros\n",
    "def read_dataset_multifile(ds_prefix, folder=os.curdir):\n",
    "    \"\"\"\n",
    "    :param ds_prefix: Prefijo de los ficheros a leer.\n",
    "    :param folder: Directorio donde se encuentran los ficheros.\n",
    "    :return: Dataframe resultante de leer los ficheros.\n",
    "    \"\"\"\n",
    "    list_df = []\n",
    "    \n",
    "    for file_name in os.listdir(folder):\n",
    "        if file_name.startswith(ds_prefix):\n",
    "            file_path = os.path.join(folder, file_name)\n",
    "            df_temp = pd.read_csv(file_path)\n",
    "            list_df.append(df_temp)\n",
    "            \n",
    "    return pd.concat(list_df, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plstrs = read_dataset_multifile(PLSTRS_PREFIX,MPD_CSV_PATH)\n",
    "\n",
    "trackid_map_dict = df_tracks[['track_uri']].to_dict()['track_uri']\n",
    "trackid_map_dict = {v: k for k, v in trackid_map_dict.items()}\n",
    "\n",
    "df_plstrs[\"track_pid\"] = df_plstrs[\"track_uri\"].map(trackid_map_dict)\n",
    "df_plstrs.drop(columns=['track_uri'], inplace=True)\n",
    "df_tracks.drop(columns=['track_uri'], inplace=True)\n",
    "df_plstrs.sort_values(['pid', 'pos'],inplace=True)\n",
    "df_plstrs.set_index('pid',inplace=True)\n",
    "df_plstrs.index.name = 'pl_pid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file_name in os.listdir(MPD_CSV_PATH):\n",
    "    if file_name.startswith(PLSTRS_PREFIX):\n",
    "        os.remove(os.path.join(MPD_CSV_PATH,file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plsarts = pd.merge(df_plstrs.reset_index(), \n",
    "                      df_tracks.reset_index()[['track_pid', 'artist_pid']], on=['track_pid'])\n",
    "df_plsarts.drop(columns=['track_pid','pos'], inplace=True)\n",
    "df_plsarts.sort_values(by=['pl_pid','artist_pid'], inplace=True)\n",
    "df_plsarts = df_plsarts.groupby(['pl_pid','artist_pid']).size().to_frame(name = 'artist_count').reset_index()\n",
    "df_plsarts.set_index('pl_pid',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plinfo_dtypes = {'modified_at' : int, 'num_albums': int, 'num_tracks': int, 'num_followers' : int,\n",
    "                 'num_edits': int, 'duration_ms' : int, 'num_artists': int}\n",
    "\n",
    "df_plsinfo = pd.read_csv(PLSINFO_FILE, dtype=plinfo_dtypes,index_col=0)\n",
    "df_plsinfo.index.name = 'pl_pid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "plinfo_test_dtypes = {'num_holdouts' : int, 'num_samples': int, 'num_tracks': int}\n",
    "\n",
    "df_plsinfo_test = pd.read_csv(PLSTESTINFO_FILE, dtype=plinfo_test_dtypes, index_col=0)\n",
    "df_plsinfo_test.index.name = 'pl_pid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artists.to_csv(ARTISTS_FILE)\n",
    "df_albums.to_csv(ALBUMS_FILE)\n",
    "df_tracks.to_csv(TRACKS_FILE)\n",
    "df_plsinfo.to_csv(PLSINFO_FILE)\n",
    "df_plsinfo_test.to_csv(PLSTESTINFO_FILE)\n",
    "df_plstrs.to_csv(PLSTRS_FILE)\n",
    "df_plsarts.to_csv(PLSTARTS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "## <font color=\"#92002A\">4 - Creación de características de playlist</font>\n",
    "<br>\n",
    "\n",
    "A continuación, vamos a convertir los títulos en etiquetas, en las que cada una corresponderá a una característica.\n",
    "\n",
    "Como podemos recordar, algunas playlists contienen emoticonos. Para este caso creamos un diccionario con el cual cambiar dichos elementos por palabras.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section21\"></a>\n",
    "### <font color=\"#B20033\">Emoticonos a texto</font>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprueba si un carácter es un emoticono\n",
    "def is_emoji(char):\n",
    "    return char in emoji.UNICODE_EMOJI['en']\n",
    "\n",
    "# Obtiene la lista de emoticonos que aparecen en un texto dado\n",
    "def get_emojis_list(text):\n",
    "    text_chars = list(text)\n",
    "    emojis_set = set()    \n",
    "    for char in text_chars:\n",
    "        if is_emoji(char):\n",
    "            emojis_set.add(char)            \n",
    "    return list(emojis_set)\n",
    "\n",
    "# Devuelve un string con los emoticonos que aparecen en un texto dado\n",
    "def get_emojis_string(text):\n",
    "    emojis_list = get_emojis_list(text)\n",
    "    return \"\".join(emojis_list)\n",
    "\n",
    "# Comprueba si un texto tiene emoticonos\n",
    "def has_emojis(text):\n",
    "    return len(get_emojis_list(text)) > 0\n",
    "\n",
    "# Elimina los emoticonos de un texto\n",
    "def remove_emojis(text):\n",
    "    return emoji.get_emoji_regexp().sub(u'', text)\n",
    "\n",
    "# De una lista de cadenas, devuelve aquellas que contienen\n",
    "# el emoticono indicado\n",
    "def get_names_with_emoji(emj, names):\n",
    "    names_list = []\n",
    "    \n",
    "    if is_emoji(emj):\n",
    "        for name in names:\n",
    "            if emj in name:\n",
    "                names_list.append(name)\n",
    "    else:\n",
    "        raise Exception(\"'{}' no es un emoji.\".format(emj))\n",
    "        \n",
    "    return names_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_words(names):\n",
    "    words_count_dict = defaultdict(int)\n",
    "    \n",
    "    for name in names:\n",
    "        words = name.split(' ')\n",
    "        for word in words:\n",
    "            # Comprobamos si la palabra no es vacia y no contiene un dígito\n",
    "            if word != '' and not any(str.isdigit(c) for c in word):\n",
    "                words_count_dict[word.lower()] += 1\n",
    "            \n",
    "    # Ordenamos las tuplas de palabras por su número de apariciones\n",
    "    words_count_dict = sorted(words_count_dict.items(), \n",
    "                              key=lambda k_v: k_v[1], \n",
    "                              reverse=True)\n",
    "    \n",
    "    # Extraemos en una lista la palabra de cada tupla\n",
    "    words_list = [word for (word,count) in words_count_dict]\n",
    "            \n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_most_frequent_words(emoj, names_list, num_words=-1):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    names = get_names_with_emoji(emoj, names_list)\n",
    "    for i, name in enumerate(names):\n",
    "        clean_name = re.sub(r'[^\\w\\s]','',remove_emojis(name).lower()).strip()\n",
    "        clean_name_tokenized = list()\n",
    "        for token in word_tokenize(clean_name):\n",
    "            if lemmatizer.lemmatize(token) not in set(stopwords.words(\"english\")):\n",
    "                clean_name_tokenized.append(lemmatizer.lemmatize(token))\n",
    "        names[i] = ' '.join(clean_name_tokenized)\n",
    "    word_list = most_frequent_words(names)[0:num_words]\n",
    "    \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emoji_dict(names_list, removable_words=[], num_tags=5):\n",
    "    names_with_emoji_list = [name for name in names_list if has_emojis(name)]\n",
    "    \n",
    "    emojis_set = set()    \n",
    "    for name in names_with_emoji_list:\n",
    "        emojis_set.update(get_emojis_list(name))    \n",
    "    \n",
    "    emoji_dict = dict()\n",
    "    for emoj in tqdm_nb(emojis_set):\n",
    "        emoji_dict[emoj] = emoji_most_frequent_words(emoj, names_with_emoji_list)\n",
    "        \n",
    "    words_count_dict = defaultdict(int)\n",
    "    \n",
    "    for _,value in emoji_dict.items():\n",
    "        for element in value:\n",
    "            words_count_dict[element] +=1\n",
    "    \n",
    "    words_count_dict = sorted(words_count_dict.items(), \n",
    "                              key=lambda k_v: k_v[1], \n",
    "                              reverse=True)\n",
    "            \n",
    "    for k,v in emoji_dict.items():\n",
    "        emoji_dict[k] = [word for word in v if word not in removable_words][0:num_tags]\n",
    "        \n",
    "    emoji_dict = {k: v for k, v in emoji_dict.items() if len(v) > 0}\n",
    "    \n",
    "    return emoji_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id=\"section31\"></a>\n",
    "### <font color=\"#B20033\">Limpieza de texto</font>\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_chars(text):\n",
    "    unwanted_chars = string.punctuation.replace(\"'\", \"\")\n",
    "    if any(x in text for x in string.punctuation):\n",
    "        for c in unwanted_chars :\n",
    "            text = text.replace(c, ' ')\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multiple_whitespaces(text):\n",
    "    return re.sub(' +', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = remove_emojis(text.lower())\n",
    "    text = remove_punctuation_chars(text)\n",
    "    text = remove_multiple_whitespaces(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id=\"section31\"></a>\n",
    "### <font color=\"#B20033\">Creación de etiquetas</font>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(cleaned_text, emojis_list=[], emojis_translation_dict=dict(), max_emoji_translations=3):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokenized_text = set(word_tokenize(cleaned_text))\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    text_tags_set = set(stemmer.stem(word) for word in tokenized_text \n",
    "                        if word not in stop_words)\n",
    "\n",
    "    if len(emojis_list) > 0 and len(emojis_translation_dict) > 0:\n",
    "        emojis_tags_set = set()\n",
    "        for emoj in emojis_list:\n",
    "            if emoj in emoji_dict:\n",
    "                emojis_tags_set.update(emojis_translation_dict[emoj][0:max_emoji_translations])\n",
    "        \n",
    "        text_tags_set = text_tags_set.union(emojis_tags_set)\n",
    "    \n",
    "    return \"|\".join(list(text_tags_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unusual_tags(name_tags_list,min_appearances=2):\n",
    "    tags_count = defaultdict(int)\n",
    "    for name_tags in name_tags_list:\n",
    "        for tag in name_tags.split('|'):\n",
    "            tags_count[tag] += 1\n",
    "\n",
    "    removable_set = set()\n",
    "    for tag,count in tags_count.items():\n",
    "        if count < min_appearances:\n",
    "            removable_set.add(tag)\n",
    "            \n",
    "    return removable_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unusual_tags(tags, unusual_tags_set):\n",
    "    tags_list = tags.split('|')\n",
    "    new_tags_list = []\n",
    "    for tag in tags_list:\n",
    "        if tag not in unusual_tags_set:\n",
    "            new_tags_list.append(tag)\n",
    "            \n",
    "    return ('|').join(new_tags_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id=\"section31\"></a>\n",
    "### <font color=\"#B20033\">Proceso final</font>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_names_df(df_names, emojis_translation_dict=dict(), remove_unusualtags=False):\n",
    "    total_steps = 3\n",
    "    if remove_unusualtags:\n",
    "        total_steps += 1\n",
    "    \n",
    "    tqdm_nb.pandas()\n",
    "    print(\"(1/{}) Cleaning playlists names\".format(total_steps))\n",
    "    df_names['clean_name'] = df_names.name.progress_apply(clean_text)\n",
    "    print(\"(2/{}) Emojis translation\".format(total_steps))\n",
    "    df_names['emojis'] = df_names.name.progress_apply(get_emojis_string)\n",
    "    print(\"(3/{}) Generating tags\".format(total_steps))\n",
    "    df_names['tags'] = df_names.progress_apply(lambda df: get_tags(df['clean_name'], df['emojis'],\n",
    "                                                                   emojis_translation_dict,\n",
    "                                                                   max_emoji_translations=3),axis=1)\n",
    "    \n",
    "    df_names.drop(columns=['emojis'], inplace=True)\n",
    "    df_names.drop(columns=['clean_name'], inplace=True)\n",
    "    \n",
    "    if remove_unusualtags:\n",
    "        print(\"(4/{}) Removing unusual tags\".format(total_steps))\n",
    "        removable_tags_set = get_unusual_tags(df_names.tags.to_list()) \n",
    "        df_names['tags'] = df_names.progress_apply(lambda df: remove_unusual_tags(df['tags'], \n",
    "                                                                                  removable_tags_set),\n",
    "                                                   axis=1)    \n",
    "    \n",
    "    return df_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74864d06d494b24a6dc3c5cfe3294ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1081 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Diccionario de emoticonos\n",
    "if os.path.isfile(EMOJI_DICT_FILE):\n",
    "    with open(EMOJI_DICT_FILE) as json_file:\n",
    "        emoji_dict = json.load(json_file)\n",
    "else:\n",
    "    names = pd.read_csv(PLSINFO_FILE, index_col=0)['name'].astype(str).to_list()\n",
    "    names = names + pd.read_csv(PLSTESTINFO_FILE, index_col=0)['name'].astype(str).to_list()\n",
    "    emoji_dict = create_emoji_dict(names, removable_words= [\"music\", \"playlist\", \"song\"])\n",
    "    with open(EMOJI_DICT_FILE, 'w') as json_file:\n",
    "        json.dump(emoji_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🎐', ['bit', 'everything', 'swavé', 'mi', 'watercolour']),\n",
       " ('🙃', ['sad', 'mood', 'love', 'feel', 'good']),\n",
       " ('♑', ['capricorn', 'gamzee', 'makara', 'ive', 'hellbent']),\n",
       " ('🌤', ['morning', 'cloud', 'day', 'summer', 'spring']),\n",
       " ('♣', ['attack', 'team', 'nigga', 'explosion', 'later']),\n",
       " ('🤶', ['christmas', 'justin', 'bieber', 'drummer']),\n",
       " ('🚂', ['train', 'soul', 'express', 'toot', 'gain']),\n",
       " ('🤰', ['pregnant', 'word'])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(emoji_dict.items())[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/3) Cleaning playlists names\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23aa2693b944ed18351c76ba0ee549d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1010000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2/3) Emojis translation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc205c5521b44a281337b8fc7256575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1010000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3/3) Generating tags\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d39e21abff4bd1a0dbc480afc7c936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1010000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DataFrame nombres\n",
    "if os.path.isfile(NAMES_FILE):\n",
    "    df_names = pd.read_csv(NAMES_FILE, index_col=0)\n",
    "else:\n",
    "    df_names = pd.read_csv(PLSINFO_FILE, index_col=0)[['name']].astype(str)\n",
    "    df_names_test = pd.read_csv(PLSTESTINFO_FILE, index_col=0)[['name']].astype(str)\n",
    "    df_names = create_names_df(pd.concat([df_names,df_names_test]), emoji_dict, remove_unusualtags=False)\n",
    "    df_names.to_csv(NAMES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pl_pid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Low viscosity vibes</td>\n",
       "      <td>viscos|low|vibe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dalanda 🐉</td>\n",
       "      <td>dalanda|imagine|game|dragon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>freeze pops</td>\n",
       "      <td>pop|freez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Golden Oldies</td>\n",
       "      <td>oldi|golden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I d◻n't g◻v◻ a d◻mn</td>\n",
       "      <td>dmn|n't|dnt|gv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name                         tags\n",
       "pl_pid                                                  \n",
       "0       Low viscosity vibes              viscos|low|vibe\n",
       "1                 dalanda 🐉  dalanda|imagine|game|dragon\n",
       "2               freeze pops                    pop|freez\n",
       "3             Golden Oldies                  oldi|golden\n",
       "4       I d◻n't g◻v◻ a d◻mn               dmn|n't|dnt|gv"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-graduation-cap\" aria-hidden=\"true\" style=\"color:#92002A\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
